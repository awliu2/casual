% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{amsmath}
\usepackage{optidef}
\usepackage{accents}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Problem Set 5: Estimation of TEs Using Matching Methods},
  pdfauthor={Tessie Dong, Derek Li, Andi Liu},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Problem Set 5: Estimation of TEs Using Matching Methods}
\author{Tessie Dong, Derek Li, Andi Liu}
\date{Due Feb 1st, 2024}

\begin{document}
\maketitle

\newlength{\dhatheight}
\newcommand{\doublehat}[1]{%
    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
    \addtolength{\dhatheight}{-0.35ex}%
    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
    \smash{\hat{#1}}}}

\noindent \textcolor{Maroon}{\textbf{Objective}: In PSet 3 you used NSW experimental data to estimate the treatment effect of the offer of training on post-training earnings. In PSet 4 you used datasets created to mimic observational data and reported estimates of the treatment effect obtained using regression-based methodologies. In this pset you complete STEP 1 in \textcolor{ForestGreen}{propensity score matching (PSM)}. Specifically, you estimate each sample unit's probability of being included in the treatment group, which we call the \textcolor{ForestGreen}{propensity score}, or \textcolor{ForestGreen}{pscore} for short. In the next PSet you use the estimated pscores to estimate the impact of the offer of training on post-intervention earnings using observational data.}

\noindent \textcolor{Maroon}{\textbf{Background}: A unit's pscore is \underline{known} when the data results from an experiment because it is set by the researcher and used to assign units to one group or the other. For example, in an RCT with random assignment (RA) the researcher flips (say) a balanced coin to decide whether to assign a unit to treated or control groups. That is, the researcher uses an assignment probability equal to 0.5. Thus, each unit's pscore is 0.5. As another example consider an RCT with conditional random assignment (CRA) such that units are either males or females and the researcher assigns males to the treated group with probability 0.5 but they assign females to the treated group with probability 0.3. Then the pscore of any male sample unit is 0.5 and the pscore of any female sample unit is 0.3.}

\noindent \textcolor{Maroon}{When the data is observational, a unit's pscore is \underline{not known} hence you need to first estimate it in order to use it in matching procedures. In this PSet you estimate the pscore based on two models: the \textcolor{ForestGreen}{Linear Probability Model (LPM)} and the \textcolor{ForestGreen}{Logit Model}. The LPM posits that, as a function of the unit's observable and predetermined characteristics $\mathbf{x}_i=(1, x_{i,1},\ldots,x_{i,K})$, the probability of ``success'' (with ``success'' = being assigned to the treatment group) is a linear-in-parameter function\footnote{This explains the name ``LPM'': it \textit{models} the \textit{probability} of “success” as a \textit{linear}-in-parameter function. The ``linearity'' in the name of this model pertains to the way in which parameters enter the specification. The OPVs may enter linearly or non-linearly. For example, say that there are two covariates $(x_{1i},x_{2i})$, then the model $\Pr(D_i=|\mathbf{x}_i=\mathbf{x})=\theta_0+\theta_1 x_{1}+\theta_2 x_{2}+\theta_3 x_{1}^2+\theta_4 ln(x_{1})$ is a LPM. This means that, when in the text we write, e.g., $\Pr(D_i=|\mathbf{x}_i=\mathbf{x})=\theta'\mathbf{x}$, the RHS variables may be transformation of the original OPVs.}:
\begin{equation}\label{eq:lpm}
\Pr(D_i=1|\mathbf{x}_i=\mathbf{x})=\theta'\mathbf{x}, \text{ for some } \theta=(\theta_0,\theta_1, \ldots, \theta_K).
\end{equation}
\noindent Instead, the Logit model posits that, as a function of the unit's characteristics, the probability of ``success'' is a non-linear function of the parameters of the form:
\begin{equation}\label{eq:logit}
\Pr(D_i=1|\mathbf{x}_i=\mathbf{x})=\frac{e^{\gamma'\mathbf{x}}}{1 + e^{\gamma'\mathbf{x}}}, \text{ for some } \gamma=(\gamma_0,\gamma_1, \ldots, \gamma_K).
\end{equation}
\noindent You know the LPM from introductory econometrics. Why? You verified in PSet 1 that: 
\begin{equation}\label{eq:meanDeqPr}
E[D_i|\mathbf{x}_i=\mathbf{x}]=\Pr(D_i=1|\mathbf{x}_i=\mathbf{x}).
\end{equation}
Taken together expressions (\ref{eq:lpm}) and (\ref{eq:meanDeqPr}) say that the mean of $D_i$ is a \underline{linear-in-parameter} function, i.e., 
\begin{equation}\label{eq:meanLPM}
E[D_i|\mathbf{x}_i=\mathbf{x}]=\theta'\mathbf{x}. 
\end{equation}
This means that the LPM is a standard multivariate linear regression model (MLRM) whose dependent variable is the binary 0/1 variable $D_i$. Thus, you can use OLS to estimate the parameters of a LPM, in your case $\theta$. In this Pset you do that and you also use a second estimation methodology suitable for linear-in-parameter models, \textcolor{ForestGreen}{Lasso regression}. (No prior knowledge of Lasso is expected, we guide you through implementation.)}

\noindent \noindent \textcolor{Maroon}{Next, using expressions (\ref{eq:logit}) and (\ref{eq:meanDeqPr}) you have:
\begin{equation}\label{eq:meanLogit}
E[D_i|\mathbf{x}_i=\mathbf{x}]=\frac{e^{\gamma'\mathbf{x}}}{1 + e^{\gamma'\mathbf{x}}}.
\end{equation}
This says that according to the Logit model the mean of $D_i$ is a \underline{non-linear} function of the parameters. Thus, you cannot apply OLS nor Lasso to estimate $\gamma$. Instead you use a methodology called \textcolor{ForestGreen}{Maximum Likelihood Estimation (MLE)}. (No prior knowledge of MLE is expected, we guide you through implementation.) By the end of this PSet you will have estimated the pscore for all the units included in the \texttt{nswpsid.csv} dataset.}
\newpage

\begin{center}
{\LARGE Part 1: Estimate the PScore Using A Linear-in-Parameter Model (65 p)}
\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  (5 p) To familiarize yourself with the LPM here you practice with this
  model in a simplified setting. Suppose that there is only one OPV
  \(x_i\), and \(x_i\) takes values \(\{1,\ldots,M\}\) (e.g., your data
  only includes education, and this OPV ranges from 1 year to 12 years).
  Suppose that you use the following LPM specification for the
  conditional expectation of treatment status as a function of
  education: \(E[D_i|x_i]=\sum_{m=1}^{M}\theta_m1[x_i=m]\). Find
  \(\hat{p}_i\) for each sample unit.
  \textcolor{gray}{\textbf{Hint}: this is a paper and pencil exercise, you do not have to write any R script. You want to suggest an estimator for $\theta=(\theta_1,\ldots,\theta_m)$, get the estimates, then use the estimates to get the implied fitted value $\hat{p}_i$ for each $i$.}

  \textbf{Solution:} By definition,
  \[ E[D_i | x_i] = \sum_{m=1}^M \theta_m1[x_i=m]\] Then, we can set
  \(x_i\) to a specific value, \(j\), where \(1 <= j <= M\),
  \[ E[D_i | x_i = j] = \theta_11[x_i = j] + \theta_21[x2 = j] + ... + \theta_M1[x_M = j] \]
  Then for all values \(m\), where \(1 <= m <= M\) such that
  \(x_m \neq j\), we observe that \(\theta_m1[x_m = j] = 0\). So we get
  \[ E[D_i | x_i = j] = \theta_j \] Then, we propose the natural
  estimator for \(\theta_j\) to be \(\hat{\theta_j}\) such that
  \[ \hat{\theta_j} = \frac{\sum_{i=1}^N D_i1[x_i = j]}{\sum_{i=1}^N1[x_i = j]}\]
  that is, \(\hat{\theta_j}\) represents the sample proportion of those
  who received treatment among the individuals within the sample who
  have the given value of \(x_i = j\). Then by the Law of Large Numbers,
  \[ \hat{\theta_j} \xrightarrow{p} E[D_i | x_i = j] = \theta_j, \forall j\]
  Then, we can find \(\hat{p_i}\) for each sample unit,
  \[ \hat{p_i} = \hat{p}(x_i) = \sum_{m=1}^M \hat{\theta_m}1[x_i = m] \]
  which we know to be a consistent estimator of \(E[D_i|x_i]\) by the
  Continuous Mapping Theorem.
\item
  (45 p) Consider the LPM introduced in expression (\ref{eq:lpm}) and
  recall the (implied) expression (\ref{eq:meanLPM}) for the conditional
  mean of treatment status. Here you estimate \(\theta\) given a
  specific form of \(\theta'\mathbf{x}_{i}\) and using the OLS
  approach.\label{item:lpm:ols}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    (10 p) Use the \texttt{nswpsid.csv} data with: 1) \texttt{treat} as
    \(D_i\); 2) a constant and the following 13 covariates as
    \(\mathbf{x}_{i}\):
    \texttt{age, agesq, edu, edusq, married, nodegree, black, hisp, re74, re75, re74sq, re75sq, u74black},
    where e.g.~\texttt{re74sq} is the square of \texttt{re74} and
    \texttt{u74black} is the interaction b/w \texttt{u74} and
    \texttt{black}. Obtain the OLS estimator of
    \(\theta=(\theta_0,\theta_1,\ldots,\theta_{13})\), denoted
    \(\hat{\theta}\).
    \textcolor{gray}{\textbf{Programming Guidance:} Declare a formula object with \texttt{as.formula( )} then use \texttt{stats::lm( )}. For later reference, in our R script we named the formula object \texttt{pscore\_formula}.}
    \label{item:lpm:ols:reg}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nswpsid }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"pset5/starter{-}files/nswpsid.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{agesq =}\NormalTok{ age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{edusq =}\NormalTok{ edu}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{re74sq =}\NormalTok{ re74}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{re75sq =}\NormalTok{ re75}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{u74black =}\NormalTok{ u74}\SpecialCharTok{*}\NormalTok{black)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pscore\_formula }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ agesq }\SpecialCharTok{+}\NormalTok{ edu }\SpecialCharTok{+}\NormalTok{ edusq }\SpecialCharTok{+} 
\NormalTok{                              married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+} 
\NormalTok{                              re74 }\SpecialCharTok{+}\NormalTok{ re75 }\SpecialCharTok{+}\NormalTok{ re74sq }\SpecialCharTok{+}\NormalTok{ re75sq }\SpecialCharTok{+}\NormalTok{ u74black)}
\NormalTok{ols }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(pscore\_formula, }\AttributeTok{data =}\NormalTok{ nswpsid)}
\NormalTok{theta\_hat }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ols}\SpecialCharTok{$}\NormalTok{coefficients)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(theta\_hat)}
\end{Highlighting}
\end{Shaded}

    \begin{longtable}[]{@{}lr@{}}
    \toprule\noalign{}
    & ols.coefficients \\
    \midrule\noalign{}
    \endhead
    \bottomrule\noalign{}
    \endlastfoot
    (Intercept) & 0.0475879 \\
    age & -0.0038278 \\
    agesq & 0.0000216 \\
    edu & 0.0327368 \\
    edusq & -0.0011982 \\
    married & -0.1344377 \\
    nodegree & 0.0654532 \\
    black & 0.0243867 \\
    hisp & 0.0912156 \\
    re74 & -0.0000009 \\
    re75 & -0.0000025 \\
    re74sq & 0.0000000 \\
    re75sq & 0.0000000 \\
    u74black & 0.5702562 \\
    \end{longtable}
  \item
    Use \(\hat{\theta}\) from \textbf{\ref{item:lpm:ols:reg}} to
    describe the change in the pscore associated with a change in one of
    the covariates. Specifically:

    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \item
      (2 p) Show that
      \(\frac{\partial Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial x_{i,k}} = \frac{\partial E[D_i|\mathbf{x}_{i}=\mathbf{x}]}{\partial x_{i,k}}= \frac{\partial \mathbf{x}'\theta}{\partial x_{i,k}} = \theta_k\)
      for any \(k >0\) and \(x_{i,k}\) that varies continuously and such
      that no other covariates mechanically changes when \(x_{i,k}\)
      changes. \textcolor{gray}{\textbf{Hint:} Use Calculus.}

      \textbf{Solution:} From \ref{eq:meanDeqPr} and \ref{eq:meanLPM} we
      know that
      \[E[D_i|\mathbf{x}_i=\mathbf{x}]=\Pr(D_i=1|\mathbf{x}_i=\mathbf{x}) = E[D_i|\mathbf{x}_i=\mathbf{x}]=\theta'\mathbf{x}.\]
      Then, we can rewrite the partial derivative as \begin{align*}
       \frac{\partial Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial x_{i,k}} &= \frac{\partial E[D_i|\mathbf{x}_{i}=\mathbf{x}]}{\partial x_{i,k}} \\
       &= \frac{\partial \mathbf{x}'\theta}{\partial x_{i,k}} \ \text{, where $\mathbf{x}'\theta = \theta'\mathbf{x}$}\\ 
       \end{align*} Furthermore, we can express \(\mathbf{x}'\theta\) as
      \[\mathbf{x}'\theta = \theta_0 + x_1 \theta_1 + \ldots + x_k \theta_k + \ldots + x_K \theta_K\]
      from which it is easy to see that the partial derivative of this
      linear equation with respect to \(x_{i,k}\), for some fixed
      \(k > 0\) and continuous \(x_{i, k}\), is \(\theta_k\).
      \hfill \(\blacksquare\) \newline
    \item
      (4 p) Write out the expression for
      \(\frac{\partial Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial \texttt{re75}}\)
      paying attention to the fact that earnings in 1975 contribute to
      two mechanically related regression covariates: \texttt{re75} and
      \texttt{re75sq}.
      \textcolor{gray}{\textbf{Hint:} Use Calculus.}\label{item:lpm:delta-pscore-re75}

      \textbf{Solution:} Similar to (i), we can express our partial
      deriative as:
      \[\frac{\partial (\theta_0 + x_1 \theta_1 + \ldots + x_k \theta_k + \ldots + x_K \theta_K)}{\partial x_{i,k}}.\]

      Specifically, these covariates include the terms \texttt{re75} and
      \texttt{re75sq}, which are the 1975 earnings and the square of the
      1975 earnings, respectively. Thus, we can express the partial
      derivative with respect to \texttt{re75} as:
      \[\frac{\partial (\theta_0 + x_1 \theta_1 + \ldots + \texttt{re75} \theta_{\texttt{re75}} + \texttt{re75}^2 \theta_{\texttt{re75sq}} + \ldots + x_K \theta_K)}{\partial \texttt{re75}}\]
      which we find equivalent to
      \(\theta_{\texttt{re75}} + 2\theta_{\texttt{re75sq}}\texttt{re75}\).
      \hfill \(\blacksquare\) \newline
    \item
      Use \(\hat{\theta}\) and the expression from
      \textbf{\ref{item:lpm:delta-pscore-re75}} to complete this
      sentence:
      ``\textit{Evaluated at average 1975 earnings, a \$10,000 increase in 1975 earnings is associated with [\dots] percentage points [lower/higher] probability of being included in the treated group}.'\,'
      \textcolor{gray}{\textbf{Hint:} Start by computing average 1975 earnings.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute average 1975 earnings:}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{mean}\NormalTok{(nswpsid}\SpecialCharTok{$}\NormalTok{re75))}
\FunctionTok{colnames}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"mean re75"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

      \begin{longtable}[]{@{}r@{}}
      \toprule\noalign{}
      mean re75 \\
      \midrule\noalign{}
      \endhead
      \bottomrule\noalign{}
      \endlastfoot
      17850.89 \\
      \end{longtable}

      First, we note that \(\theta_{\texttt{re75sq}}\) is neglible, that
      is to say, given our partial derivative
      \(\theta_{\texttt{re75}} + 2\theta_{\texttt{re75sq}}\texttt{re75} = \theta_{\texttt{re75}}\),
      we do not need to account for the starting earnings amount in our
      calculation to determine the change in pscore given a change in
      earnings. As \(\theta{\texttt{re75}}\) = -0.0000025, we find that
      \(P(D_\texttt{re75}=1)\) decreases by
      \(-0.0000025 * 100000 = 0.025\), i.e 2.5 percentage points.
    \end{enumerate}
  \item
    (6 p) As mentioned above, when \(D_i\) indicates whether a unit
    belongs to the treated or the control group we call it the
    \textcolor{ForestGreen}{treatment status indicator} and we call the
    probability of ``success'\,', namely
    \(Pr(D_i=1|\mathbf{x}_i=\mathbf{x})\), the
    \textcolor{ForestGreen}{propensity score}. Use the regression output
    from \textbf{\ref{item:lpm:ols:reg}} to obtain
    \textcolor{ForestGreen}{fitted values}\footnote{Consider a generic MLRM regression framework where $y_i$ denotes the dependent variable, $\mathbf{x}_i$ are the covariates (including a constant), and $\beta$ are the coefficients. Let $\hat{\beta}$ denote the OLS estimates. We define the fitted value of the dependent variable as $\hat{y}_i \equiv \hat{\beta}'\mathbf{x}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i}+\ldots+\hat{\beta}_K x_{i,K}$.},
    denoted \(\hat{p}_i\). Note: Interpret \(\hat{p}_i\) as the
    estimated probability that a unit \(i\) with pre-treatment
    characteristics \(\mathbf{x}_i\) is treated.
    \textcolor{gray}{\textbf{Programming Guidance:} Apply \texttt{stats::predict( )} to the object returned by \texttt{stats::lm( )} to get fitted values.}\label{item:lpm:ols:fitted}
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ols, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{3}
  \tightlist
  \item
    (2 p) Add \(\hat{p}_i\) obtained in
    \textbf{\ref{item:lpm:ols:fitted}} to your dataframe as a column.
    \textcolor{gray}{\textbf{Programming Guidance:} Use \texttt{dplyr::mutate( )}}.
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nswpsid }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(nswpsid, }\AttributeTok{p\_hat =}\NormalTok{ p\_hat)}
\end{Highlighting}
\end{Shaded}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{4}
  \tightlist
  \item
    (4 p) The LPM has one disadvantage: \(\hat{p}_i\) is not guaranteed
    to be between 0 and 1 (the range of values a probability may take).
    For how many units is \(\hat{p}_i\) obtained in
    \textbf{\ref{item:lpm:ols:fitted}} negative or larger than 1?
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of units with p\_hat \textless{} 0}
\NormalTok{n\_less }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(nswpsid[nswpsid}\SpecialCharTok{$}\NormalTok{p\_hat }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, ])}
\CommentTok{\# number of units with p\_hat \textgreater{} 1}
\NormalTok{n\_greater }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(nswpsid[nswpsid}\SpecialCharTok{$}\NormalTok{p\_hat }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{, ])}
\NormalTok{n\_total }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(nswpsid)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(n\_less, n\_greater, n\_total)}
\FunctionTok{colnames}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"\# $p\_hat \textless{} 0$"}\NormalTok{, }\StringTok{"\# p\_hat \textgreater{} 0"}\NormalTok{, }\StringTok{"\# total"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(df, }\AttributeTok{align =} \StringTok{"c"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}ccc@{}}
  \toprule\noalign{}
  \# \(p_hat < 0\) & \# p\_hat \textgreater{} 0 & \# total \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  1076 & 0 & 2675 \\
  \end{longtable}
\item
  (15 p) In \textbf{\ref{item:lpm:ols}} you estimated \(\theta\) using
  OLS. Here you use \textcolor{ForestGreen}{Lasso} regression.
  Specifically,

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    (10 p) Obtain \(%
        \settoheight{\dhatheight}{\ensuremath{\hat{\theta}}}%
        \addtolength{\dhatheight}{-0.35ex}%
        \hat{\vphantom{\rule{1pt}{\dhatheight}}%
        \smash{\hat{\theta}}}\).
    \textcolor{gray}{\textbf{Programming Guidance:} In our R script the formula is called \texttt{pscore\_formula} and the dataframe \texttt{dt\_psid}. To implement the Lasso approach you need to take a few steps: 1) prepare two objects \texttt{x} and \texttt{y} storing, respectively, the covariates and the dependent variable of the regression; 2) find the value of $\lambda$ to plug in the minimization problem (\ref{eq:lasso:objfnc}); 3) use \texttt{glmnet::glmnet(..., alpha = 1)} to fit the Lasso model (glm stands for \textcolor{ForestGreen}{generalized linear model}). Specifically, you want to incorporate in your R script the following lines of code:}
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimate the pscore using Lasso}
\CommentTok{\# Predictor variables}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{model.matrix}\NormalTok{(pscore\_formula, }\AttributeTok{data =}\NormalTok{ nswpsid)[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }
\CommentTok{\# Outcome variable}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ nswpsid}\SpecialCharTok{$}\NormalTok{treat }
\CommentTok{\# Find the best lambda using cross{-}validation}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{cv\_lasso }\OtherTok{\textless{}{-}}\NormalTok{ glmnet}\SpecialCharTok{::}\FunctionTok{cv.glmnet}\NormalTok{(x, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\CommentTok{\# Display the best lambda value}
\NormalTok{cv\_lasso}\SpecialCharTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0003247039
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Apply Lasso}
\NormalTok{lasso\_propensity }\OtherTok{\textless{}{-}}\NormalTok{ glmnet}\SpecialCharTok{::}\FunctionTok{glmnet}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{,  }\AttributeTok{standardize =} \ConstantTok{TRUE}\NormalTok{,}
                                   \AttributeTok{lambda =}\NormalTok{ cv\_lasso}\SpecialCharTok{$}\NormalTok{lambda.min)}
\end{Highlighting}
\end{Shaded}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    (5 p) Compare \(\hat{\theta}\) and \(%
        \settoheight{\dhatheight}{\ensuremath{\hat{\theta}}}%
        \addtolength{\dhatheight}{-0.35ex}%
        \hat{\vphantom{\rule{1pt}{\dhatheight}}%
        \smash{\hat{\theta}}}\). Are there coefficient estimates that
    Lasso sets to zero but OLS does not?
    \textcolor{gray}{\textbf{Programming Guidance:} View the coefficient estimates from \texttt{lm( )} and \texttt{glmnet:glmnet( )} by applying method \texttt{coef( )}, e.g. \texttt{coef(lasso\_propensity)} where in our script \texttt{lasso\_propensity} is the object returned by \texttt{glmnet::glmnet( )}}.
  \end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_hat\_hat }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(lasso\_propensity)}
\NormalTok{coef\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(theta\_hat\_hat[, }\DecValTok{1}\NormalTok{], theta\_hat[, }\DecValTok{1}\NormalTok{])}
\FunctionTok{colnames}\NormalTok{(coef\_df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Lasso"}\NormalTok{, }\StringTok{"OLS"}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(coef\_df)}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}lrr@{}}
  \toprule\noalign{}
  & Lasso & OLS \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  (Intercept) & 0.0658750 & 0.0475879 \\
  age & -0.0023128 & -0.0038278 \\
  agesq & 0.0000000 & 0.0000216 \\
  edu & 0.0259393 & 0.0327368 \\
  edusq & -0.0009330 & -0.0011982 \\
  married & -0.1351921 & -0.1344377 \\
  nodegree & 0.0612463 & 0.0654532 \\
  black & 0.0234110 & 0.0243867 \\
  hisp & 0.0882811 & 0.0912156 \\
  re74 & -0.0000008 & -0.0000009 \\
  re75 & -0.0000025 & -0.0000025 \\
  re74sq & 0.0000000 & 0.0000000 \\
  re75sq & 0.0000000 & 0.0000000 \\
  u74black & 0.5709564 & 0.5702562 \\
  \end{longtable}

  From the above results, we can see that lasso zeros out the
  \texttt{agesq} coefficient, whereas OLS does not. It also assigns less
  weight to age and education covariates than the OLS model.
\end{enumerate}

\begin{center}
{\LARGE Part 2: Estimate the Propensity Score Using the Logit Model (35 p)}
\end{center}

\noindent 

\textcolor{Maroon}{\textbf{Background: Maximum Likelihood Estimation}: MLE returns the values of the parameters that maximize the joint distribution of the data, that is, that make the data as ``most likely'' of having been generated by a parametric model.\footnote{You should have covered MLE estimation of LRMs in introductory econometrics, and MLE more generally in any introductory statistics class. In case you didn't, we offer a brief introduction as applied to the setting under consideration.} It is useful to clearly differentiate between the RVs in a RS and their realized values: denote $\tilde{D_i}$ the realized value of $D_i$ and similarly $\tilde{\mathbf{x}}_i$ the realized value of $\mathbf{x}_i$. The joint distribution of the data is therefore: $\Pr(D=\tilde{D}|\mathbf{X}=\tilde{\mathbf{X}};\gamma)$ where $D$ collects all the $D_i$'s, $\tilde{D}$ collects all the $\tilde{D_i}$'s, $\mathbf{X}$ collects all the $\mathbf{x}_i$'s, and $\tilde{\mathbf{X}}$ collects all the $\tilde{\mathbf{x}}_i$'s. $\Pr(D=\tilde{D}|\mathbf{X}=\tilde{\mathbf{X}};\gamma)$ is called the \textcolor{ForestGreen}{Likelihood function} and is often denoted $\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})$ to underscore that we want to think of it as a function of the parameters $\gamma$ for given realization of the data. When we work with a RS we have independence, which means that the joint distribution factors into the product of $N$ terms: 
\begin{equation}
\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})= \prod_{i=1}^{N}\Pr(D_i=\tilde{D}_i|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)
\end{equation}
Because $\tilde{D}_i$ takes value either 0 or 1, we can also write:
\begin{equation}
\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})= \prod_{i=1}^N\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)^{\tilde{D}_i}\times \Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)^{1-\tilde{D}_i}.
\end{equation}
It is common to work with the logarithm of this function, called the \textcolor{ForestGreen}{Log-Likelihood function}:
\begin{equation}
\begin{aligned}
\ell(\gamma; \tilde{D}, \tilde{\mathbf{X}}) &\equiv ln(\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})) \\
&=\sum_{i=1}^{N}\Big[\tilde{D}_i \times ln(\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma))+(1-\tilde{D}_i)\times ln(\Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma)) \Big], \\
&=\sum_{i \text{ s.t.} \tilde{D}_i=1}ln(\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma))+\sum_{i \text{ s.t.} \tilde{D}_i=0}ln(\Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma)).
\end{aligned}
\end{equation}
Then the MLE estimator of $\gamma$ is: 
\begin{argmaxi}
{\gamma}{\ell(\gamma; \tilde{D}, \tilde{\mathbf{X}}).}{\label{eq:mle:objfnc}}{\hat{\gamma}=}
\end{argmaxi}
In the special case of the Logit model $\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)=\frac{e^{\tilde{\mathbf{x}}_i'\gamma}}{1+e^{\tilde{\mathbf{x}}_i'\gamma}}$ and $\Pr(D_i=0_i|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)=\frac{1}{1+e^{\tilde{\mathbf{x}}_i'\gamma}}$. Hence the MLE estimator of $\gamma$ in the Logit model under consideration is:
\begin{argmaxi}
{\gamma}{\sum_{i \text{ s.t. }\tilde{D}_i=1} ln\big[\frac{e^{\tilde{\mathbf{x}}_i'\gamma}}{1+e^{\tilde{\mathbf{x}}_i'\gamma}} \big] + \sum_{i \text{ s.t. }\tilde{D}_i=0} ln\big[\frac{1}{1+e^{\tilde{\mathbf{x}}_i'\gamma}} \big].}{\label{eq:logit:mle:objfnc}}{\hat{\gamma}=}
\end{argmaxi}
Thus, to find $\hat{\gamma}$ you solve the system of first order conditions; this system has as many unknowns as there are elements in $\gamma$.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  (5 p) To familiarize yourself with MLE, here you practice with this
  estimation approach in a simplified setting. Suppose that there are no
  OPVs and that your sample has size \(N=100\), and 10 units have
  \(\tilde{D}_i=1\) and 90 units have \(\tilde{D}_i=0\). Assume that
  each \(D_i\) has a Bernoulli distribution with parameter \(\gamma\).
  Write the likelihood function, take FOCs, and solve the FOC system
  (which has just one equation in one unknown) by paper and pencil. What
  you obtain is the MLE estimate of \(\gamma\).
  \textcolor{gray}{\textbf{Hint}: You know, intuitively, that the answer must be $\hat{\gamma}=\frac{1}{10}$.}

  \textbf{Solution:} Given this setup, our likelihood function can be
  written as: \[
           \mathcal{L}(\gamma; \tilde{D}) = \prod_{i=1}^{N}\Pr(D_i=1; \gamma)^{\tilde{D}_i}\times \Pr(D_i=0; \gamma)^{1-\tilde{D}_i}
       \] \[
           \text{where } \Pr(D_i=1; \gamma) = \gamma, \text{ and } \Pr(D_i=0; \gamma) = 1 - \gamma, \text{ since } D_i \overset{\mathrm{iid}}{\sim} B(N, \gamma)
       \] We ignore conditioning on \(\mathbf{x}\) since we have no OPVs
  in this simplified setting. We can write log-likelihood function as:
  \[
           \ell(\gamma; \tilde{D}) \equiv \sum_{i \text{ s.t } \tilde{D}_i = 1} ln(\gamma) \ \ + \sum_{i \text{ s.t } \tilde{D}_i = 0} ln(1 - \gamma)
       \] Taking the FOCs and solving for our parameter
  \(\hat{\gamma}\), we have: \begin{align*}
           \frac{\partial \ell(\gamma; \tilde{D})}{\partial \gamma} = & \frac{10}{\hat{\gamma}} - \frac{90}{1 - \hat{\gamma}} = 0 \\
           \implies &\frac{10}{\hat{\gamma}} = \frac{90}{1 - \hat{\gamma}} \ \implies \hat{\gamma} = \frac{1}{10}
       \end{align*}

  which is the MLE estimate that we expected. \hfill \(\blacksquare\)
\item
  (20 p) Consider the Logit model introduced in expression
  (\ref{eq:logit}). We mentioned that the Logit model yields a
  conditional mean of \(D_i\) that is not linear in parameters. Here you
  estimate \(\gamma\) using the MLE approach.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    (10 p) Continue using the \texttt{nswpsid.csv} dataset, and the
    variable definitions provided in \textbf{\ref{item:lpm:ols:reg}}.
    Obtain the MLE estimator of
    \(\gamma= (\gamma_0,\gamma_1,\dots,\gamma_K)\), denoted
    \(\hat{\gamma}\).
    \textcolor{gray}{\textbf{Programming Guidance:} Use \\ \texttt{stats::glm(pscore\_formula, family = binomial( ), data = psid\_tb)} where \texttt{family = binomial( )} tells R that you want to estimate a Logit model; \texttt{psid\_tb} is the name of the dataframe in our R script.}\label{item:logit:reg}
    \newpage

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psid\_tb }\OtherTok{\textless{}{-}}\NormalTok{ nswpsid}
\NormalTok{logit\_model }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{glm}\NormalTok{(pscore\_formula, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{( ), }\AttributeTok{data =}\NormalTok{ psid\_tb)}
\NormalTok{mle\_coef }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(logit\_model}\SpecialCharTok{$}\NormalTok{coefficients)}
\FunctionTok{colnames}\NormalTok{(mle\_coef) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"MLE Coefficients"}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(mle\_coef)}
\end{Highlighting}
\end{Shaded}

    \begin{longtable}[]{@{}lr@{}}
    \toprule\noalign{}
    & MLE Coefficients \\
    \midrule\noalign{}
    \endhead
    \bottomrule\noalign{}
    \endlastfoot
    (Intercept) & -7.5524581 \\
    age & 0.3305734 \\
    agesq & -0.0063429 \\
    edu & 0.8247711 \\
    edusq & -0.0483153 \\
    married & -1.8840624 \\
    nodegree & 0.1299868 \\
    black & 1.1329613 \\
    hisp & 1.9627618 \\
    re74 & -0.0001047 \\
    re75 & -0.0002172 \\
    re74sq & 0.0000000 \\
    re75sq & 0.0000000 \\
    u74black & 2.1370420 \\
    \end{longtable}
  \item
    (10 p) Use the estimated coefficients to describe the change in the
    pscore associated with a change in one of the covariates.
    Specifically:

    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \item
      (3 p) Show that
      \(\frac{\partial \Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial x_{i,k}}=\frac{\partial \frac{e^{\mathbf{x}'\gamma}}{1+ e^{\mathbf{x}'\gamma}}}{\partial x_{i,k}}= \gamma_k \frac{e^{\mathbf{x}'\gamma}}{(1+e^{\mathbf{x}'\gamma})^2}\)
      for any \(k >0\) and \(x_{i,k}\) that varies continuously and such
      that no other covariates mechanically changes when \(x_{i,k}\)
      changes.
      \textcolor{gray}{\textbf{Hint:} Use Calculus.}\label{item:logit:partial}

      \textbf{Solution:} From the problem setup, we are given that under
      the logit model that

      \[
           \Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x}) = \frac{e^{\mathbf{x}'\gamma}}{1+ e^{\mathbf{x}'\gamma}}
       \]

      thus the partial derivative can be expressed as

      \begin{align*}
           \frac{\partial \frac{e^{\mathbf{x}'\gamma}}{1+ e^{\mathbf{x}'\gamma}}}{\partial x_{i,k}} = \frac{\left(\frac{\partial \mathbf{x}'\gamma}{\partial x_{i, k}}\right)e^{\mathbf{x}'\gamma}}{\left(1 + e^{\mathbf{x}'\gamma}\right)^2} \text{ after applying quotient rule and simplifying.} \\
       \end{align*}

      And the term
      \(\frac{\partial \mathbf{x}'\gamma}{\partial x_{i, k}}\) can be
      rewritten as: \[
           \frac{\partial (\gamma_0 + x_1 \gamma_1 + \ldots + x_k\gamma_k + \ldots + x_K\gamma_K)}{\partial x_{i, k}} = \gamma_k
       \]

      Thus we arrive at the solution
      \(\gamma_k \ \frac{e^{\mathbf{x}'\gamma}}{\left(1 + e^{\mathbf{x}'\gamma}\right)^2}\)
      \hfill \(\blacksquare\)
    \end{enumerate}

    \newpage

    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \setcounter{enumiii}{1}
    \item
      (7 p) Write out the expression for
      \(\frac{\partial \Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial \texttt{re75}}\)
      paying attention to the fact that earnings in 1975 contribute to
      two mechanically related regression covariates: \texttt{re75} and
      \texttt{re75sq}.
      \textcolor{gray}{\textbf{Hint:} Use Calculus.}\label{item:logit:delta-pscore-re75}

      \textbf{Solution:} From above, we know that we can express this
      probability as \[\
          \frac{\left(\frac{\partial \mathbf{x}'\gamma}{\partial \texttt{re75}}\right)e^{\mathbf{x}'\gamma}}{\left(1 + e^{\mathbf{x}'\gamma}\right)^2}
      \]

      We once again expand the partial derivative term:

      \begin{align*}
          \frac{\partial \mathbf{x}'\gamma}{\partial \texttt{re75}} &= \frac{\partial (\gamma_0 + x_1 \gamma_1 + \ldots + x_{\texttt{re75}} \ \gamma_{\texttt{re75}} + x_{\texttt{re75sq}} \ \gamma_{\texttt{re75sq}} + \ldots + x_K\gamma_K)}{\partial \texttt{re75}} \\
          &= \frac{\partial (\gamma_0 + x_1 \gamma_1 + \ldots + x_{\texttt{re75}} \ \gamma_{\texttt{re75}} + x_{\texttt{re75}}^2 \ \gamma_{\texttt{re75sq}} + \ldots + x_K\gamma_K)}{\partial \texttt{re75}} \\
          &= \gamma_{\texttt{re75}} + 2\gamma_{\texttt{re75sq}} \ x_{\texttt{re75}}
      \end{align*}

      Thus, we can express our partial derivative as \[
           (\gamma_{\texttt{re75}} + 2\gamma_{\texttt{re75sq}} \ x_{\texttt{re75}}) \cdot \frac{e^{\mathbf{x}'\gamma}}{\left(1 + e^{\mathbf{x}'\gamma}\right)^2}.
      \] \vspace{1em}
    \end{enumerate}
  \end{enumerate}
\item
  (4 p) Use the regression output from \textbf{\ref{item:logit:reg}} to
  obtain \textcolor{ForestGreen}{fitted values} denoted
  \(\hat{p}_i^{Logit}\). Note that you interpret \(\hat{p}_i^{Logit}\)
  as the estimated probability that a unit \(i\) with pre-treatment
  characteristics \(\mathbf{x}_i\) is treated.
  \textcolor{gray}{\textbf{Programming Guidance:} Apply \texttt{stats::predict( )} with argument \texttt{type = ``response"} to the object returned by \texttt{stats::glm( )}}.\label{item:logit:fitted}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_hat\_logit }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit\_model, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\item
  (2 p) Add \(\hat{p}_i^{Logit}\) to your dataframe as a column (you
  will use it in PSet 6 to estimate average TEs by PSM).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psid\_tb }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(psid\_tb, }\AttributeTok{p\_hat\_logit =}\NormalTok{ p\_hat\_logit)}
\end{Highlighting}
\end{Shaded}
\item
  (4 p) The Logit has one advantage over the LPM: \(\hat{p}_i^{Logit}\)
  is guaranteed to be strictly between 0 and 1. Why is that?
\end{enumerate}

\end{document}

---
title: "Problem Set 6: Estimation of TEs Using Matching Methods"
author: "Tessie Dong, Derek Li, Andi Liu"
date: Due Feb 15th, 2024
output:
    pdf_document:
        keep_tex: true
        extra_dependencies: ['amsmath', 'optidef', 'accents','caption']
    html_document: default
colorlinks: true
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Change root.dir to match your working directory location
knitr::opts_knit$set(root.dir = "~/Documents/casual/pset6")
rmarkdown::pandoc_available()
library(tidyverse)
library(utils)
library(dplyr)
library(systemfit)
library(knitr)
library(readr)
```

\input{figures/premise.tex}

\newpage

## Part 1: Naive Stratification Matching on the Propensity Score (50 Points)
\input{figures/part1_background.tex}

1. (5 p) Let $\widehat{p}_i$ denote the Logit-based estimate of the pscore for unit $i$ which you obtained in PSet 5. Drop all sample units whose $\widehat{p}_{i}$ falls outside of the common support. Why may a researcher want to trim the sample in this way? How many control (treated) units do you drop? Are you surprised? In all subsequent questions you use the resulting \textcolor{ForestGreen}{trimmed sample}. \textcolor{gray}{\textbf{Programming Guidance:} Use \texttt{dplyr::filter()} to drop rows from a dataframe. Save the resulting dataframe as a new object; in our R script it is called \texttt{psid\_trimmed}.}\label{item:pscore-trim}

```{r}
load_data <- function(filename){
  dt <- data.table::as.data.table(
    readr::read_csv(filename,
                     col_names = TRUE,
                     col_types = readr::cols(
                       treat = readr::col_integer(),
                       age = readr::col_integer(),
                       edu = readr::col_integer(),
                       black = readr::col_integer(),
                       hisp = readr::col_integer(),
                       married = readr::col_integer(),
                       re74 = readr::col_double(),
                       re75 = readr::col_double(),
                       re78 = readr::col_double(),
                       u74 = readr::col_integer(),
                       u75 = readr::col_integer(),
                       nodegree = readr::col_integer()
                     ))
  )
  return(dt)
}
```

```{r}
filename_psid = "starter-files/nswpsid.csv"

dt_psid <- load_data(filename = filename_psid)

dt_psid[ , `:=` (agesq = age^2, edusq = edu^2, 
                 re74sq = re74^2, re75sq = re75^2, u74black=u74*black)]

pscore_formula = as.formula(treat ~ age + agesq + edu + edusq + 
                              married + nodegree + black + hisp + 
                              re74 + re75 + re74sq + re75sq + u74black)
```


```{r}
logit_obj <- stats::glm(pscore_formula, family=binomial(), data = dt_psid)
```

```{r}
dt_psid$logit_propensity <- predict(logit_obj, type="response")
```

```{r}
control <- dt_psid %>% filter(treat == 0)
treatment <- dt_psid %>% filter(treat == 1)
```

```{r}
p_cl = min(control$logit_propensity)
p_cu = max(control$logit_propensity)

p_tl = min(treatment$logit_propensity)
p_tu = max(treatment$logit_propensity)
```

```{r}
trimmed_df <- subset(dt_psid, logit_propensity >= 
              max(p_cl, p_tl) & logit_propensity <= min(p_cu, p_tu))
```

```{r}
length(control$logit_propensity)
- length((trimmed_df %>% filter(treat == 0))$logit_propensity)
```
```{r}
length(treatment$logit_propensity)
- length((trimmed_df %>% filter(treat == 1))$logit_propensity)
```

**Solution:** A researcher may want to trim the sample in this way because if we are interested in matching p-scores between treatment and control groups, dropping p-scores that are too low or too high allows the OPVs between the groups to be more balanced. This is because we are removing units with certain covariates that are unlikely or very likely to be treated, in order to avoid cases in which there are no good matches for a unit from the treatment group within the control group (or vice versa).

We dropped 1344 control units and 6 treated units. This is not surprising given the difference in the bounds, where $\hat{p}^{T,l}$ is almost $10e6$ times as large compared to $\hat{p}^{C,l}$ whilst the upper bounds are within $0.001$ of one another. However, a significant portion of the dropped control units have p scores that are very close to zero - i.e they would likely not be matched to a treated unit.

2. (5 p) Figure \ref{fig:propensity_vs_earnings} plots \texttt{re78} (the outcome metric) against $\widehat{p}_{i}$, separately for the treated and the control groups. Each panel also includes a curve that displays the fitted non-parametric regression of \texttt{re78} on $\widehat{p}_{i}$ i.e., a smoothed estimate of the mean of earnings conditional on the value of the pscore. Take for example the curve in the Treated panel, it connects earnings values constructed as follows. Pick a value of the pscore, e.g., $p=0.89567$. Identify the (for example) 10 closest pscore values in the treated sample. Using these 11 points, regress \texttt{re78} on a constant and the pscore values, giving more weight to the pscore values closest to $p=0.89567$. The implied fitted value of earnings at $p=0.89567$ is the value depicted on the curve.

a. Reproduce Figure \ref{fig:propensity_vs_earnings}. Test your understanding of the Loess smoothing method by varying the value of \texttt{span}. \textcolor{gray}{\textbf{Programming Guidance:} Use the script below which assumes that the trimmed dataframe is \texttt{trimmed\_df} and the Logit-based estimates are in column \texttt{pscore}. \texttt{ggplot2::ggplot()} produces a scatter plot. \href{https://ggplot2.tidyverse.org/reference/geom_smooth.html}{\texttt{geom\_smooth}}\texttt{(method = ``loess'', span = 0.5, method.args = list(degree = 1))} overlays a smoothed curve: (i) the \texttt{method} argument picks the LOWESS (LOcally WEighted Scatter-plot Smoother) smoothing method, i.e., the R function \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess}{\texttt{stats::loess()}}; (ii) the \texttt{span} argument indicates the proportion of the total number of points that contribute to each local fitted value; (iii) the \texttt{method.args} argument lists additional arguments passed on to the function \texttt{stats::loess()}, in particular \texttt{degree = 1} picks a polynomials of degree 1 (which is just a line). Here is a \href{https://ggplot2.tidyverse.org/reference/geom_smooth.html}{link} to some examples.}

  **Discussion of span:** Using the provided R script, we find that in general, decreasing the value of `span` results in a more over-fit line, and increasing its value results in a smoother line. This lines up with documentation regarding the `span` parameter: it is the proportion of total number of points that contribute to each local fitted value, and thus, using less points for each fitted value leads to over fitting, and using more points leads to a smoother line. We provide two plots for comparison: the first with `span = 0.5`, and the second with `span = 0.25`.
    
```{r, echo=FALSE}
# Draw scatter plot of post-intervention earnings
# and the Logit-based pscore, by group;
# overlay smooth local regression line.
plot_df <- trimmed_df %>%
dplyr::filter(re78 < 20000) %>% # drop outliers for plotting purposes
dplyr::mutate(treat = factor(ifelse(treat == 1, "Treated", "Control")))
p <- ggplot2::ggplot(plot_df, ggplot2::aes(x = logit_propensity, y = re78)) +
ggplot2::facet_grid(~treat) +
ggplot2::geom_point() + 
ggplot2::scale_y_continuous(breaks=seq(0,20000,by=1000)) +
ggplot2::geom_smooth(method = "loess", formula = y ~ x, span = 0.5, 
                                        method.args = list(degree = 1)) +
ggplot2::ylab("Real Earnings in 1978") +
ggplot2::xlab("Estimated Propensity Score") +
ggplot2::labs(caption = "Data Source: NSW-PSID1, span = 0.5") +
theme_bw()

p
```

```{r, echo=FALSE}
# Draw scatter plot of post-intervention earnings
# and the Logit-based pscore, by group;
# overlay smooth local regression line.
plot_df <- trimmed_df %>%
dplyr::filter(re78 < 20000) %>% # drop outliers for plotting purposes
dplyr::mutate(treat = factor(ifelse(treat == 1, "Treated", "Control")))
p <- ggplot2::ggplot(plot_df, ggplot2::aes(x = logit_propensity, y = re78)) +
ggplot2::facet_grid(~treat) +
ggplot2::geom_point() + 
ggplot2::scale_y_continuous(breaks=seq(0,20000,by=1000)) +
ggplot2::geom_smooth(method = "loess", formula = y ~ x, span = 0.25, 
                                        method.args = list(degree = 1)) +
ggplot2::ylab("Real Earnings in 1978") +
ggplot2::xlab("Estimated Propensity Score") +
ggplot2::labs(caption = "Data Source: NSW-PSID1, span = 0.25") +
theme_bw()

p
```

b. (3 p) Can you eyeball estimate\textbf{s} of the TE of the offer of training from these figures? Explain.  
        **Solution:** From the two smoothed cruves on the graphs, we can roughly eyeball that the TEs are not homogenous across different pscores. For example, we eyeball that for pscores < 0.5, the estimates of the TE are roughly 0 as there does not seem to be a significant difference between the real earnings in '78 for the control and treated groups. However, for pscores that are > 0.5, we eyeball that the estimates of TE are positive (e.g. for pscore = 0.75, the TE seems to be ~1.5k), implying that there is a positive impact of the training offer on real earnings in '78 for those pscores.

3. (40 p) We can do better than eyeballing TEs. You are now ready to implement \textcolor{ForestGreen}{stratification matching}.
    
a. (5 p) Add a column to your dataframe called \texttt{stratum} that takes values from 1 to 10 corresponding to 10 equally spaced intervals with \texttt{stratum=1} if $0<\widehat{p}_{i}\leq 0.1$, \texttt{stratum=2} if $0.1<\widehat{p}_{i}\leq 0.2$, and so on.\footnote{\textit{Strata} is the plural of \textit{stratum}, both are Latin words.} What is the distribution of control and treated observation across strata? Are all strata \textcolor{ForestGreen}{populated}? \textcolor{gray}{\textbf{Programming Guidance:} To bin the values of a column, use the R base function \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/.bincode( )}{\texttt{.bincode}} with break points at 0, 0.1, 0.2 etc. up to 1. To add a column to a dataframe, use \texttt{dplyr::mutate( )}. To produce summary statistics by group, use \texttt{dplyr::group\_by( )}, and pipe the result to \texttt{dplyr::summarize( )}. In our R script we produce the following summary statistics within each stratum: count of treated units, count of control units, minimum pscore value, maximum pscore value.}
```{r}
b <- c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)

trimmed_df <- dplyr::mutate(trimmed_df, 
                            stratum = .bincode(trimmed_df$logit_propensity, b))

strata <- trimmed_df %>% 
  dplyr::group_by(trimmed_df$stratum) %>% 
  dplyr::summarize(count_of_treated = length(which(treat == 1)), 
                  count_of_control = length(which(treat == 0)), 
                  proportion_treated = mean(treat))

knitr::kable(strata)
```

**Discussion:** The distribution of control and treated observation across strata is shown in the table above. We see that within stratum 9 (between 0.8 and 0.9), there are no control units. Besides that stratum, all others are populated by both treated and control units, albeit with different distributions. We also observe that there is a general trend of increasing proportion of treated units as we move from stratum 1 to stratum 10.

b. (25 p) Test that the average of each pretreatment variable is the same for treated and control units within each stratum: the pre-treatment variables are \texttt{age, edu, married, nodegree, black, hisp, re74, re75, u74black}. Can you think of a reason why you may want to run this test? Comment on your findings. \textcolor{gray}{\textbf{Programming Guidance:} Use SUR estimation as you did in PSet 3, but now do this within each stratum. Skip strata that do not have both control and treated units. Within some of the strata, some of the pre-treatment variables are perfectly collinear or do not vary. This causes \texttt{systemfit::systemfit( )} to produce an error. To avoid it, you first identify which of the pre-determined variables exhibit the problem, then you exclude them from the list of dependent variables of the SUR system, then you invoke \texttt{systemfit::systemfit( )}. To loop over strata, use the \texttt{for} loop: \texttt{for(stratrum in 1:10) \{ $<$ script $>$\}}. Below is a user-defined function that returns the names of the collinear columns in a dataframe, feel free to use it:}

  **Solution:** We may want to run this test to ensure that the pretreatment variables are balanced across the treated and control groups within each stratum. This is important because even though it may be the case that the pre-treatment variables are balanced across the entire dataset, we want to ensure that they are still balanced even after we stratify by pscore. Since we are estimating ATT using difference in post-treatment earnings within each of our stratum, we would like that OPV balance still holds within each of these sub-groups of data. Per the results below, we see that most of the stratum (that have both treated and control individuals) have balanced OPVs at a 0.05 level. However, we see that strata 5 and 8 in particular have p-values less than 0.05, indicating that at least one of the OPVs is significantly different across the treatment/control groups and we cannot be confident that the OPVs are balanced within these strata. We might consider that the small sample sizes of these strata may be causing the imbalance in OPVs, and perhaps we could consider dropping these strata, or combining them with other strata to ensure that we have balanced OPVs within each stratum.

```{r}
# Declare a function that identifies perfectly collinear covariates.
# Note: this fnc is roughly equivalent to _rmcoll in STATA.
rmcoll <- function(df, colnames = names(df)) {
  # Arguments:
  # - df: A data frame.
  # - colnames: A list of column names.
  # Returns:
  # A list with the column names of collinear variables.
    df_ <- df %>% dplyr::select(one_of(colnames))
    cc <- coef(lm(rep(1, nrow(df_)) ~ ., data = df_))
    return(names(cc)[is.na(cc)])
} # end fnc rmcoll
```

```{r}
stratum_results <- list()
opvs <- c("age", "edu", "married", "nodegree", "black",
            "hisp", "re74", "re75", "u74black")
for(stratum_i in 1: 10){
  subset <- trimmed_df %>% filter(stratum == stratum_i)
  # Skip strata that do not have either no control or no treated units
  if (sum(subset$treat == 0) <= 0 || 
      sum(subset$treat == 1) <= 0) {
    next
  }
  
  non_collinear <- setdiff(opvs, rmcoll(subset, opvs))

  sur_system <- list()
  null_system <- list()
  for (v in non_collinear) {
    sur_system[[v]] <- formula(paste(v, "~ treat"))
    null_system[[v]] <- formula(paste(v, "~ 1"))
  }
  # null_system <- map(vars, function(x) formula(paste(x, "~ 1")))
  sur_fit <- systemfit::systemfit(formula=sur_system, data=subset, method="SUR")
  null_fit <- systemfit::systemfit(formula=null_system, data=subset, method="SUR")

  lrtest_obj <- lrtest(null_fit, sur_fit)

  row <- data.frame(stratum = stratum_i, p_value = lrtest_obj$'Pr(>Chisq)'[2])
  stratum_results <- append(stratum_results, list(row))
}
```

```{r, echo=FALSE}
bound_df <- bind_rows(stratum_results)
knitr::kable(bound_df, row.names = FALSE,
            caption = "P-values of the test of the equality of the OPVs 
            across the treated and control groups within each stratum.")
```

c. (10 p) Let $N_{s}^{T}$ denote the number of treated units in stratum $s$ with $s=1,...,10$. Let $N^{T}=\sum_{s=1}^{10}N_{s}^{T}$. Let $\overline{re78}_{s}^{1}$ (respectively, $\overline{re78}_{s}^{0}$) denote average post-treatment earnings for treated (respectively, control) units in stratum $s$. Estimate the ATT of the offer of training using stratification matching estimator (\ref{strata_ATT}). Explain in plain English why this is an estimator of the ATT. \textcolor{gray}{\textbf{Programming Guidance:} To compute a weighted average use \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/weighted.mean}{\texttt{stats::weighted.mean( )}} where you let the argument \texttt{w} be a vector with the counts of treated units in each stratum. Verify that you get \$1,563.}\label{item:naive-ATT-estimate}

    
    \begin{equation}
    \widehat{ATT}^{SM}=\sum_{s=1}^{10}\left( \frac{N_{s}^{T}}{N^{T}}\right) \left( \overline{re78}_{s}^{1}-\overline{re78}_{s}^{0}\right) 
    \text{.}\label{strata_ATT}
    \end{equation}
    
    
```{r}

te <- c()
w <- c()

for(i in 1: 10) {
  
  if(i == 9){
    next
  }
  
  stratum_df <- trimmed_df %>% filter(stratum == i)
  stratum_treat <- stratum_df %>% filter(treat == 1)
  stratum_control <- stratum_df %>% filter(treat == 0)
  re78_1bar <- mean(stratum_treat$re78)
  re78_0bar <- mean(stratum_control$re78)
  stratum_te <- re78_1bar - re78_0bar
  
  te <- c(te, stratum_te)
  
  weight <- nrow(stratum_treat)
  w <- c(w, weight)
}

att_sm <- round(weighted.mean(te, w/sum(w)), digits = 2)
att_sm


```
  
**Solution:** From above, we see that our calculated estimate of the ATT to be \$1562.73 which rounds to \$1563, as desired. This is an estimate of the ATT because we are computing the treatment effect within the sub-population of the treated individuals. We can think of each calculate TE within stratum as a CATE (conditional upon the stratum), and then we performed a weighted sum of these CATEs (weighed by proportion of treated within each stratum/total treated in the sample), thus our final estimator is an estimator of the ATT.

## Part 2: Fancier Matching On the PScore (50 Points)

\input{figures/part2_background.tex}
\newpage

4. (6 p) Denote the outcome variable by $y$. Show that under unconditional random assignment (RA), $\widehat{ATT}^{m}=\overline{y}^{1}-\overline{y}^{0},$ $\forall m\in \left\{ NNM,RM,KM\right\}$. That is, if there are no observable confounders, all three estimators reduce to the Treatment-Control Comparison estimator. \textcolor{gray}{\textbf{Hint:} Assign treatment by the flip of a balanced coin, i.e., let $\widehat{p}_{i}=0.5$ for all $i$. Then, simplify expressions (\ref{ATT_m}) and (\ref{w_ij}). Do read carefully the background information presented above.}

**Solution:** First leverage that $\hat{p}_i = 0.5 \ \forall i$ and (2) to show that 
\begin{align*}
  C^{NNM}(i) &= \{j \in C : |\hat{p}_j - \hat{p}_i| \leq |\hat{p}_{j'} - \hat{p}_i| \ \forall j' \in C\}\\
  &= \{j \in C : |0.5 - 0.5| \leq |0.5 - 0.5| \}\\
  &= \{j \in C : 0 \leq 0 \}\\ 
  &= \{j \in C : True \}\\ 
  &= C 
\end{align*}

Then recognize that with the same assumption in (3), we show that 
\begin{align*}
  C^{RM}(i) &= \{j \in C : |\hat{p}_j - \hat{p}_i| < r\}\\
  &= \{j \in C : |0.5 - 0.5| < r \}\\
  &= \{j \in C : 0 < r \}\\ 
  &= C \text{, given that we choose $r > 0$}. 
\end{align*}

We have from (\ref{ATT_m}) and (\ref{w_ij}) for $m \in \{NNM, RM\}$ that: 

\begin{align*}
    \widehat{ATT}^{m} &= \frac{1}{N^T}\sum_{i\in T} \left[y_i - \sum_{j \in C^{m}(i)} w_{ij}^m y_j\right]\\ 
                      &= \frac{1}{N^T}\sum_{i \in T} \left[  y_i - \sum_{j \in C} \frac{1}{N^C} y_j\right] \text{, by previous derivation showing that $C^{m}(i) = C$, }\\
                      &= \frac{1}{N^T}\sum_{i \in T}  \left[ y_i - \frac{1}{N^C} \sum_{j \in C\left( i\right)} y_j \right] \text{ from lin. of $\sum$}\\
                      &= \frac{1}{N^T}\sum_{i \in T}  \left[ y_i - \bar{y}^0 \right] \text{, by definition, }\\
                      &= \frac{1}{N^T}\sum_{i \in T} [y_i] - \frac{1}{N_T} \sum_{i\in T} [\bar{y}^0]\text{, by $\sum$ properties, }\\
                      &= \bar{y}^1 - \frac{1}{N^T} \cdot N^T \cdot \bar{y}^0 \text{, by summation over a constant, } \\
                      &= \bar{y}^1 - \bar{y}^0. 
\end{align*}

Now we show that this is also true for the KM estimator. We have from
(\ref{ATT_m}) and (\ref{w_ij}) that: \begin{align*}
    \widehat{ATT}^{KM} &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \sum_{j \in C^{KM}\left( i\right)}\frac{K\left( \frac{\hat{p}_{j} - \hat{p}_{i}}{h}\right)}{\sum_{k \in C}K \left( \frac{\hat{p}_{k}-\hat{p}_{i}}{h}\right)} y_j \right ] \\
  \end{align*}

Under RA, we can set probabilities
\(\hat{p_k}, \ \hat{p_i}, \ \hat{p_j}\) equal to 0.5 and simplify the
expression to:

\begin{align*}
    \widehat{ATT}^{KM} &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \sum_{j \in C^{KM}\left( i\right)}\frac{K\left(0\right)}{\sum_{k \in C} K \left(0\right)} y_j \right] \\
                       &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \sum_{j \in C^{KM}\left( i\right)}\frac{K\left(0\right)}{N^c \cdot K \left(0\right)} y_j \right] \text{ from summation over a constant,} \\
                       &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \frac{1}{N^c} \sum_{j \in C^{KM}\left( i\right)} y_j \right] \text{ via lin. of $\sum$,} \\
                       &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \frac{1}{N^c} \sum_{j \in C} y_j \right] \\
                       &= \frac{1}{N^T} \left[ \sum_{i \in T} y_i - \bar{y}^0 \right] = \overline{y}^{1} - \overline{y}^{0}. & \blacksquare
  \end{align*}

5. (9 p) Show that $\widehat{ATT}^{m}$ in expression (\ref{ATT_m}) rewrites as:

\begin{equation} \label{ATT_m_bis}
\widehat{ATT}^{m} =\overline{y}^{1}-\overline{y}_{m}^{0} \ \forall m\in \left\{ NNM,RM,KM\right\},
\end{equation}

\noindent where:

\begin{equation}\label{ATT_m_bis_w}
\overline{y}^{1} =\frac{1}{N^{T}}\sum_{j \in T}y_{j}, \text{     and      }  \overline{y}_{m}^{0} =\frac{1}{N^{C}}\sum_{j \in C}\pi_{j}^{m}y_{j}.
\end{equation}

\noindent Use expression (\ref{ATT_m_bis}) to describe in plain English the essence of each matching approach.  \textcolor{gray}{\textbf{Hint:} Find the expression for $\pi _{j}^{m}$ in expression (\ref{ATT_m_bis_w}) for each $m\in \left\{NNM,RM,KM\right\} $}.


6. (3 p) With the trimmed sample from Part 1, produce the 1:1 NNM estimate of ATT using \href{https://www.rdocumentation.org/packages/Matching/versions/4.9-6/topics/Match}{\texttt{Matching::Match( )}} from the package \href{https://cran.r-project.org/web/packages/Matching/Matching.pdf}{\texttt{Matching}}. Verify that it coincides with that presented in Table \ref{tab:Tab25-4_2}. \textcolor{gray}{\textbf{Note:} Table \ref{tab:Tab25-4_2} also reports estimates based on 3 other PSM approaches (you are not asked to implement them). ``Adaptive Stratification'' is a refinement of the ``Naive Stratification'' approach that you implemented in Part 1: strata are created iteratively (as opposed to being fixed ahead of time) so as to ensure balance within each stratum.}  \textcolor{gray}{\textbf{Programming guidance:} \texttt{Matching::Match()} takes 3 dataframes as inputs, \texttt{Y}, \texttt{Tr} and \texttt{X}. \texttt{Y} and \texttt{Tr} are columns \texttt{re78} and \texttt{treat} respectively in your dataframe, and \texttt{X} is the column that stores the Logit estimates of the pscore because you want to match on the estimated pscore. Specify \texttt{M=1} and \texttt{estimand=``ATT''} so that you estimate ATT using 1:1 matching. Use \texttt{summary()} on the object returned, which is a list of 23 elements.} \label{item:estimate-att-ncs}


\begin{table}[h]
\centering
\begin{tabular}{ccc}
\hline \hline
\textbf{Matching Estimator} & \textbf{Estimate (\$/year)} & \textbf{SE (\$/year)} \\ \hline
1:1 Nearest-Neighbor Matching with Replacement (NNM)       &  490.39 & 1,929.6  \\
Radius Matching with $r=0.0001$ (RM)  &  -5,546.139 &  4,614.773 \\
Gaussian Kernel Matching (KM)        &  1,537.947 & 861.329  \\ 
Adaptive Stratification Matching     &  2,208.603 &  855.168 \\ \hline
\end{tabular}
\caption{Estimates of ATT based on four pscore-matching estimators.}
\label{tab:Tab25-4_2}
\end{table}

```{r}
Y <- trimmed_df$re78
Tr <- trimmed_df$treat
X <- trimmed_df$logit_propensity

matched <- Matching::Match(Y,Tr,X,M=1,estimand="ATT")

nnm_df <- data.frame(matched[c('est', 'se')])
colnames(nnm_df) <- c('NNM Estimate', 'NNM Std. Error')

summary_matched <- summary(matched)

knitr::kable(nnm_df)
```
From these outputs, we can verify that our results match exactly those provided.

7. (16 p) Consider the \textcolor{ForestGreen}{matched data}, i.e., the collection of pairs such that each pair is a treated unit and a matched control.
  
a. (2 p) How many treated units are paired with at least one control unit? 
      ```{r}
      num_treated_paired = length(unique(matched$index.treated))
      num_treated_paired
      ```
    **Solution:** 179 treated units are paired with at least one control unit.
      
  b. (2 p) How many treated units are paired with exactly one control unit? \newline
    **Solution:** 179 treated units are paired with at least one control unit. Since we set M=1 in the Match function, each treated unit is matched with exactly one control unit. Therefore, the number of treated units paired with exactly one control unit would be the same as the number of treated units that were paired as in part (a).

  c. (2 p) What percentage of control units serve as a match to at least one treated unit? \newline
      **Solution:** About 46.68\% of control units serve as a match to at least one treated unit.
      ```{r}
      num_control_matched = length(unique(matched$index.control))
      total_control_units = sum(Tr == 0)
      control_matched = (num_control_matched / total_control_units) * 100
      control_matched
      ```
    
    
    
  d. (2 p) Take the treated units that are paired with exactly 1 control unit, how many distinct control units serve as matches to these treated units? Are you surprised? \newline
    **Solution:** 535 distinct control units serve as matches to these treated units. Initially, this was surprising because all the treated units are paired with exactly one control unit, but there are more matched control units than matched treated units.
      ```{r}
      num_distinct_controls_for_treated = length(unique(matched$index.control))
      num_distinct_controls_for_treated 
      ```

  e. (2 p) The algorithm matches one treated unit with 209 distinct control units. How do you make sense of this? \newline
    **Solution:** This can happen because the many control units might have identical or very similar pscores relative to a treated unit. The matching algorithm handles ties by matching the treated unit to all equally good control matches (with the same pscore), this could result in a large number of matches for a single treated unit.
      
  f. Make sense of the line ``\texttt{Matched number of observations (unweighted). 700}'' in Figure \ref{fig:nnm_output}. 
      \begin{figure}[h]
      \centering
      \captionsetup{width=.6\textwidth}
      \includegraphics[width=0.4\textwidth]{figures/q7_f.png}
      \caption{Snapshot of output produced by running \texttt{symmary()} on the object returned by \texttt{Matching::Match()} in \textbf{\ref{item:estimate-att-ncs}}.}
      \label{fig:nnm_output}
      \end{figure}
        ```{r}
        paste("length of index.treated:", length(matched$index.treated), 
              "index.control:", length(matched$index.control))
        ```
      **Solution:** This line means that there are 700 matched pairs. This does not take into account that one treated unit might be matched with multiple control units. We can obtain this result from the above code and verifying that both vectors have 700 elements.
      
  g. (2 p) Use the matched data to ``manually'' compute the ATT estimate. \newline
      **Solution:** The manually computed ATT estimate is 490.39.
        ```{r}
        sum(((trimmed_df$'re78')[matched$index.treated] - 
            (trimmed_df$'re78')[matched$index.control]) * matched$weights) / 179
        ```
      

  h. (2 p) Run \texttt{Matching::Match()} with \texttt{ties = FALSE}. Why do you get "\texttt{Matched number of observations (unweighted). 179}?" \newline
  **Solution:** I get "Matched number of observations (unweighted). 179" because setting ties = FALSE instructs the  algorithm not to match treated units to more than one control unit when there are ties in the matching criterion (e.g. distance based on propensity score). So we get exactly 179 matched pairs, i.e exactly the number of treated units in the sample.
      ```{r}
      matched_notie <- Matching::Match(Y,Tr,X,M=1,estimand="ATT",ties = FALSE)
      (summary(matched_notie))
      ```

\newpage 
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{\texttt{num\_matches}} & \textbf{\texttt{num\_treated}} & \textbf{\texttt{min\_pscore}} & \textbf{\texttt{avg\_pscore}} & \textbf{\texttt{med\_pscore}} & \textbf{\texttt{max\_pscore}} \\ \hline
209 & 1 & 0.000652575 & 0.000652575 & 0.000652575 & 0.000652575 \\ \hline
129 & 1 & 0.003290713 & 0.003290713 & 0.003290713 & 0.003290713 \\ \hline
70 & 1 & 0.005570029 & 0.005570029 & 0.005570029 & 0.005570029 \\ \hline
35 & 2 & 0.007592412 & 0.007776318 & 0.007776318 & 0.007960224 \\ \hline
9 & 1 & 0.055467716 & 0.055467716 & 0.055467716 & 0.055467716 \\ \hline
7 & 1 & 0.028241882 & 0.028241882 & 0.028241882 & 0.028241882 \\ \hline
5 & 2 & 0.07015485 & 0.082214016 & 0.082214016 & 0.094273183 \\ \hline
4 & 2 & 0.046380154 & 0.061290142 & 0.061290142 & 0.076200129 \\ \hline
3 & 3 & 0.133671949 & 0.688292564 & 0.965164821 & 0.966040924 \\ \hline
2 & 14 & 0.127383235 & 0.369653196 & 0.336881761 & 0.949418099 \\ \hline
1 & 151 & 0.141036436 & 0.753198139 & 0.900426437 & 0.972637597 \\ \hline
\end{tabular}

\caption{Descriptive statistics for the matched data. The column \texttt{num\_matches} lists the (distinct) count of control units matched to a treated unit, from largest to smallest. The column \texttt{num\_treated} reports the number of treated units with the corresponding count of matched controls. The sum of this column is 179, i.e., the number of treated units in the trimmed sample. The sum of the row-wise product of the first two columns is 700. The remaining columns present min/average/median and max value of the pscore for the treated units with the corresponding count of matched controls. Example A: 1 treated unit is matched to 209 distinct control units, this unit's pscore is 0.000652575. Example B: 14 treated units are each matched to exactly two control units, the maximum pscore among these 14 units is 0.949418099. Example C: 151 treated units are each matched to exactly one control unit, the average of these treated units' pscores is 0.753198139.}
\label{tab:matched-set-ds}
\end{table}


8. (10 p) Use \texttt{Matching::MatchBalance()} to document balance in the covariates (those in the Logit specification for the pscore model) before and after matching. Comment on your findings: a) Are covariates more/less balanced after matching? b) Are covariates in the matched sample sufficiently balanced to make you trust that the NNM estimator is not confounded by differences in observables? \textcolor{gray}{\textbf{Programming guidance:} Use 500 bootstrap replication, i.e., set \texttt{nboots=500}. Say that you name \texttt{nnm\_balance} the object (a list of lists) returned by the call to \texttt{Matching::MatchBalance()}. Apply \texttt{names()} to see the named element of the list. To check that your implementation is correct: the averages of \texttt{age} in the trimmed sample before matching and in the matched sample (i.e., after matching) are, respectively:
    \begin{itemize}
    \item \texttt{nnm\_balance\$BeforeMatching[[age\_loc]]\$mean.Tr = 25.76536}, 
    \item \texttt{nnm\_balance\$BeforeMatching[[age\_loc]]\$mean.Co = 30.96161},
    \item \texttt{nnm\_balance\$AfterMatching[[age\_loc]]\$mean.Tr = 25.76536},
    \item \texttt{nnm\_balance\$AfterMatching[[age\_loc]]\$mean.Co = 26.1724}
    \end{itemize}
where, in our script, \texttt{age\_loc} is the number that identifies the location of the information for the covariate \texttt{age}.}

**Solution:** We use the below script to document balance in the covariates before and after matching, and verify that the averages of \texttt{age} in the trimmed sample before matching and in the matched sample (i.e., after matching) are as expected. We choose to hide the rest of the output for convenience of the grader.

In general the covariates seem to be significantly more balanced than before, with our T-test p-values increasingly drastically, moving from values that were near 0 to consistently failing to reject the null hypothesis that the covariates are balanced between the treated and control groups.
Some variables, like `black` and `nodegree` didn't quite move to above the 0.05 thresholds, however, they are significantly more balanced after matching. 
Overall, the covariances in the matched sample seem to be sufficiently balanced to make us trust that the NNM estimator is not confounded by differences in most observable pre-treatment variables, with the exception of the `hisp` covariate which still exhibits significant imbalance at the 0.05 level.
```{r, results=FALSE}
formula <- as.formula("treat ~ 1 + age + I(agesq) + edu + I(edusq) + 
                       married + nodegree + black + hisp + re74 +
                       re75 + I(re74sq) + I(re75sq) + u74:black")

nnm_balance <- Matching::MatchBalance(formula, 
                                      data=trimmed_df,
                                      match.out=matched,
                                      nboots=500)
nnm_balance$BeforeMatching[[1]]$mean.Tr # 25.76536
nnm_balance$BeforeMatching[[1]]$mean.Co # 30.96161
nnm_balance$AfterMatching[[1]]$mean.Tr # 25.76536
nnm_balance$AfterMatching[[1]]$mean.Co # 26.1724
```

9. (6 p) The estimate of ATT based on the experimental data is \$1,794 with SE \$633 (see PSet 3). Consider the estimates of ATT in Table \ref{tab:Tab25-4_2}: What are the takeaways? Taking inspiration from Imbens (2015)'s recommendations, are there actions you may want to take to gain more confidence in your ability to learn about the true causal impact of the offer of training via matching when you only have access to observational data?

**Discussion:** We discuss two major takeaways and their implications from this exercise in estimating the ATT. 

Firstly, we find that in general, our point estimates of ATT and its SE varies significantly across the different matching estimators and algorithms being used. This is a potential cause for concern, as it suggests that our estimates are sensitive to the choice of matching algorithm. Working through multiple different estimators and exploring their theoretical background as well as their empirical results leaves us with a better understanding of the various strengths and weakness associated, and more importantly, with the conclusion that often times, there is not a single "best" matching estimator to default to. 

This realization leads us to our second main takeaway: beyond understanding the matching estimators and their various implementations and limitations, we also need to be aware of nuances present in the dataset we are working with, and how these nuances may affect the performance of the matching estimators. For example, we found that the covariates in the matched sample seem to be sufficiently balanced to make us trust that the NNM estimator is not confounded by differences in most observable pre-treatment variables, with the exception of the `hisp` covariate which still exhibits significant imbalance at the 0.05 level. Indeed, it is tests such as the stratified balance tests that we performed in this assignment that can allow us to gain additional insights into our data's "shape" and "style", and in turn allow us to choose one (or multiple) matching estimator(s) most appropriate for the context of our investigation.

In the context of Imbens (2015)'s recommendations, we would want to consider the following actions to gain more confidence in our ability to learn about the true causal impact of the offer of training via matching when we only have access to observational data:

  1. We would want to consider the trial and/or use of multiple matching estimators to understand the sensitivity of our results to the choice of matching algorithm. This would allow us to understand the robustness of our results and the extent to which our results are driven by the choice of matching algorithm.

  2. We would want to perform trimming of the sample to remove extreme observations that may be driving our results. This would mitigate p-score imbalances and thus improve the quality of matches and thus the performance of our matching estimators.

  3. Imbens suggests a step-wise MLE approach for estimating pscores - we would want to consider this approach to ensure that our pscores are estimated as accurately as possible. The more accurate our pscores, the better our matching estimators will perform (regardless of which matching algorithm we might choose).

\noindent \textcolor{Maroon}{\textbf{Background: Maximum Likelihood Estimation}: MLE returns the values of the parameters that maximize the joint distribution of the data, that is, that make the data as ``most likely'' of having been generated by a parametric model.\footnote{You should have covered MLE estimation of LRMs in introductory econometrics, and MLE more generally in any introductory statistics class. In case you didn't, we offer a brief introduction as applied to the setting under consideration.} It is useful to clearly differentiate between the RVs in a RS and their realized values: denote $\tilde{D_i}$ the realized value of $D_i$ and similarly $\tilde{\mathbf{x}}_i$ the realized value of $\mathbf{x}_i$. The joint distribution of the data is therefore: $\Pr(D=\tilde{D}|\mathbf{X}=\tilde{\mathbf{X}};\gamma)$ where $D$ collects all the $D_i$'s, $\tilde{D}$ collects all the $\tilde{D_i}$'s, $\mathbf{X}$ collects all the $\mathbf{x}_i$'s, and $\tilde{\mathbf{X}}$ collects all the $\tilde{\mathbf{x}}_i$'s. $\Pr(D=\tilde{D}|\mathbf{X}=\tilde{\mathbf{X}};\gamma)$ is called the \textcolor{ForestGreen}{Likelihood function} and is often denoted $\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})$ to underscore that we want to think of it as a function of the parameters $\gamma$ for given realization of the data. When we work with a RS we have independence, which means that the joint distribution factors into the product of $N$ terms: 
\begin{equation}
\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})= \prod_{i=1}^{N}\Pr(D_i=\tilde{D}_i|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)
\end{equation}
Because $\tilde{D}_i$ takes value either 0 or 1, we can also write:
\begin{equation}
\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})= \prod_{i=1}^N\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)^{\tilde{D}_i}\times \Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)^{1-\tilde{D}_i}.
\end{equation}
It is common to work with the logarithm of this function, called the \textcolor{ForestGreen}{Log-Likelihood function}:
\begin{equation}
\begin{aligned}
\ell(\gamma; \tilde{D}, \tilde{\mathbf{X}}) &\equiv ln(\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})) \\
&=\sum_{i=1}^{N}\Big[\tilde{D}_i \times ln(\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma))+(1-\tilde{D}_i)\times ln(\Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma)) \Big], \\
&=\sum_{i \text{ s.t.} \tilde{D}_i=1}ln(\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma))+\sum_{i \text{ s.t.} \tilde{D}_i=0}ln(\Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma)).
\end{aligned}
\end{equation}
Then the MLE estimator of $\gamma$ is: 
\begin{argmaxi}
{\gamma}{\ell(\gamma; \tilde{D}, \tilde{\mathbf{X}}).}{\label{eq:mle:objfnc}}{\hat{\gamma}=}
\end{argmaxi}
In the special case of the Logit model $\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)=\frac{e^{\tilde{\mathbf{x}}_i'\gamma}}{1+e^{\tilde{\mathbf{x}}_i'\gamma}}$ and $\Pr(D_i=0_i|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)=\frac{1}{1+e^{\tilde{\mathbf{x}}_i'\gamma}}$. Hence the MLE estimator of $\gamma$ in the Logit model under consideration is:
\begin{argmaxi}
{\gamma}{\sum_{i \text{ s.t. }\tilde{D}_i=1} ln\big[\frac{e^{\tilde{\mathbf{x}}_i'\gamma}}{1+e^{\tilde{\mathbf{x}}_i'\gamma}} \big] + \sum_{i \text{ s.t. }\tilde{D}_i=0} ln\big[\frac{1}{1+e^{\tilde{\mathbf{x}}_i'\gamma}} \big].}{\label{eq:logit:mle:objfnc}}{\hat{\gamma}=}
\end{argmaxi}
Thus, to find $\hat{\gamma}$ you solve the system of first order conditions; this system has as many unknowns as there are elements in $\gamma$.}

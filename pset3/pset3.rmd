---
title: "Problem Set 3"
author: "Tessie Dong, Derek Li, Andi Liu"
date: Due Jan 18th, 2024
output:
    pdf_document:
        keep_tex: true
        extra_dependencies: ['amsmath']
    html_document: default
colorlinks: true
---

<!-- R library setup -->
```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rmarkdown::pandoc_available()
library(tidyverse)
library(utils)
library(systemfit)
```


\textbf{Part 1: Describe the Data} \newline Combine the NSW data in `nswre74_control.csv` and `nswre74_treated.csv` and complete Table \ref{tab:Tab_NSW_1}. Note that variables `1-10` are $\color{ForestGreen}{predetermined}$, i.e., capture characteristics determined at or before treatment assignment; some of these variables are background characteristics (e.g., \texttt{edu}), others capture a subject's pre-RCT labor market experience (e.g., \texttt{u75}). \texttt{re78} is the observed outcome variable. \texttt{treat} is the indicator of treatment status.

```{r}
# load and combine the data sets into one dataframe
df1 <- utils::read.csv(file = "nswre74_control.csv")
df2 <- utils::read.csv(file = "nswre74_treated.csv")
df <- rbind(df1, df2)
```

```{r}
# count units in each sample
dplyr::tally(dplyr::group_by(df, treat))
```



```{r}
# generate mean summary statistics for each variable and treatment
dplyr::summarise_all((dplyr::group_by(df, treat)), list(mean))
```

A completed version of Table 1 is provided on the following page.
\newpage


\begin{table}[!ht!]
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Variable} & \textbf{Variable}    & \textbf{Variable}    & \multicolumn{2}{c}{\textbf{Sample Average}}    \\ \cline{4-5} 
\textbf{Counter} & \textbf{Name}       & \textbf{Definition}                        & \textbf{Treated} & \textbf{Control} \\ \hline
1 & \texttt{age}      & Age in years                      & 25.8        & 25.1       \\
2 & \texttt{edu}      & Education in years                & 10.3        & 10.1       \\
3 & \texttt{nodegree} & 1 if education $<12$              & 0.708       & 0.835      \\
4 & \texttt{black}    & 1 if Black                        & 0.843       & 0.827      \\
5 & \texttt{hisp}     & 1 if Hispanic                     & 0.0595      & 0.108      \\
6 & \texttt{married}  & 1 if married                      & 0.189       & 0.154      \\
7 & \texttt{u74}      & 1 if unemployed in '74            & 0.708       & 0.75       \\
8 & \texttt{u75}      & 1 if unemployed in '75            & 0.6         & 0.685      \\
9 & \texttt{re74}     & Real earnings in '74 (in '82 \$)  & 2096        & 2107       \\
10 & \texttt{re75}    & Real earnings in '75 (in '82 \$)  & 1532        & 1267       \\
\hline 
11 & \texttt{re78}    & Real earnings in '78 (in '82 \$)  & 6349        & 4555       \\
12 & \texttt{treat}   & 1 if received offer of training   & 1           & 0          \\
\hline
Sample Size                                             &&& 185         & 260              \\ \hline
\end{tabular}
\caption{Descriptive statistics for the NSW data by group.}
\label{tab:Tab_NSW_1}
\end{table}

\textbf{Part 2: Test Balance}

1. Test \textcolor{ForestGreen}{balance} for each of the 10 OPVs in Table (\ref{tab:Tab_NSW_1}), i.e.,  test that each variable's mean is the same in the control and treated groups. Do so by running 10 simple linear regressions (SLR) specifications. Use a 5\% significance level and look at the relevant 10 t-tests, comment on your findings. \textcolor{gray}{\textbf{Hint}: Each OPV is the dependent variable in its regression equation. All 10 SLR models have the same covariates.}

```{r}
ols_p_values <- list()
ols_coefficients <- list()
ols_se <- list()
vars <- names(df)[2:12]
for (var in vars) {
    formula <- stats::formula(paste(var, "~treat"))
    lm_model <- lm(formula = formula, data = df)
    ols_p_values[[var]] <- summary(lm_model)$coefficients[2, 4]
    ols_coefficients[[var]] <- summary(lm_model)$coefficients[2, 1]
    ols_se[[var]] <- summary(lm_model)$coefficients[2, 2]
}
# print(ols_p_values)
# print(ols_coefficients)
# print(ols_se)
```

<!-- 
    "age p-value: 0.264764"
    "edu p-value: 0.135411"
    "black p-value: 0.649493"
    "hisp p-value: 0.076474"
    "married p-value: 0.327408"
    "nodegree p-value: 0.001398"
    "re74 p-value: 0.982318"
    "re75 p-value: 0.382254"
    "re78 p-value: 0.004788"
    "u74 p-value: 0.326209"
    "u75 p-value: 0.065469" -->


\begin{table}
\centering
\begin{tabular}{ccccc}
\hline
& \textbf{Variable} & \textbf{Coefficient} & \textbf{se} & \textbf{p-value} \\ \hline
1 & \texttt{age} & 0.7623701 & 0.6827511 & 0.264764 \\
2 & \texttt{edu} & 0.2574844 & 0.1721353 & 0.135411 \\
3 & \texttt{nodegree} & -0.1265073 & 0.0393452 & 0.001398 \\
4 & \texttt{black} & 0.01632017 & 0.03588617 & 0.649493 \\
5 & \texttt{hisp} & -0.04823285& 0.0271632 & 0.076474 \\
6 & \texttt{married} & 0.03534304 & 0.03604844 & 0.327408 \\
7 & \texttt{u74} & -0.04189189& 0.0426221 & 0.326209 \\
8 & \texttt{u75} & -0.08461538& 0.04582176 & 0.065469 \\
9 & \texttt{re74} & -11.45296 & 516.478 & 0.982318 \\
10 & \texttt{re75} & 265.1463 & 303.1555 & 0.382254 \\ \hline
\end{tabular}
\caption{Coefficients, standard errors, and p-values for the t-tests of balance for each OPV.}
\end{table}



Table 2 shows the p-values for the t-tests of balance for each OPV. \newline 
We can see that for most of our variables, the p-value is greater than 0.05, so we fail to reject the null hypothesis that the means are the same in the control and treated groups. However, for \texttt{nodegree} we reject the null hypothesis that the means are the same in the control and treated groups, and we cannot claim that these groups are balanced for this variable.

\newpage
     
2. Testing balance as done in Q1 suffers from the so called \href{https://en.wikipedia.org/wiki/Multiple_comparisons_problem}{\textcolor{ForestGreen}{``multiple comparisons''}} or \textcolor{ForestGreen}{``multiple testing'' problem} which occurs when one considers a set of statistical inferences simultaneously. The \href{https://explainxkcd.com/wiki/index.php/882:_Significant}{problem} emerges because as more variables are compared, it becomes more likely that the treatment and control groups appear to differ on at least one attribute \textit{by random chance alone}. To deal
with this problem we use an estimation methodology called \textcolor{ForestGreen}{SUR estimation} and then test just one hypothesis, the \textit{joint} hypothesis that all OPVs are balanced, i.e., their means are the same in the two groups. SUR stands for \textcolor{ForestGreen}{seemingly unrelated regression} and it is a special case of \textcolor{ForestGreen}{feasible Generalized Least Squares (GLS) estimation}. Instead of estimating the coefficients \textit{equation-by-equation} by OLS (and done in Q1 you combine the 10 equations in a system of equations and estimate the coefficients present in all the equations jointly, accounting for the fact that the unobservables may be correlated across equations within an individual (we continue to assume that they are uncorrelated across units). After estimation, you use standard testing procedures to test the \textit{joint} hypothesis that the slope coefficients in all the equations of the SUR system are zero. This joint test is a test of covariate balance that does not suffer from the ``multiple testing'' problem.


a. Estimate the SUR system. Are the estimated coefficients and their SEs different from those obtained in Q1? Comment. \textcolor{gray}{\textbf{Hint}: In 2 situations there is no efficiency payoff to GLS versus OLS: 1) the unobservables are uncorrelated across equations within an individual; and 2) the equations have identical covariates.}

```{r, results=FALSE}
# create a list of formulas for each equation
formulas <- list()
vars <- names(df)[2:12]
for (var in vars) {
  formulas[[var]] <- formula(paste(var, "~treat"))
}
```

```{r}
# estimate the SUR model
sur_fit <- systemfit::systemfit(formula = formulas, data = df, method = "SUR")
```

```{r}
# print the coefficients and SE of the SUR model
i <- 1
while (i <= 11){
  print(sprintf("%s: %f, %f",
                vars[i],
                summary(sur_fit)$coefficients[i * 2, 1],
                summary(sur_fit)$coefficients[i * 2, 2]))
  i <- (i + 1)
}
```

```{r}
# print the coefficients and SE of the OLS model
for (var in vars)
{
  print(sprintf("%s: %f, %f ", var, ols_coefficients[var], ols_se[var]))
}
```


<!-- TODO: Interpret the results: why are the estimated coefficents and SEs the same between OLS and GLS?-->
The estimated coefficients and SEs are the same across OLS and GLS estimations for all the OPVs. This result may be due to the fact that the equations have the same covariate, i.e. \texttt{treat}. 

b. Test \textit{joint} balance by testing the \textcolor{ForestGreen}{joint hypothesis} that the coefficients of \texttt{treat} are zero in all the equations of the system. Spell out null and alternative hypotheses, comment on findings, and verify the test's value and p-value ``manually.'' \textcolor{gray}{\textbf{Hint}: Use the \href{https://en.wikipedia.org/wiki/Likelihood-ratio_test}{\textcolor{ForestGreen}{Likelihood Ratio (LR) test}} where the unrestricted model is the system of equations with \texttt{treat} as covariate, and the restricted model is the system with no regression covariates, i.e., with only the constant term. To verify the value of the test use its algebraic expression. Recall that the test has a $\chi^{2}_{df}$ distribution with $df$ = number of restrictions.}

```{r}
# create a list of formulas null_system with only the constant
null_formulas <- list()
for (var in vars) {
  null_formulas[[var]] <- formula(paste(var, "~1"))
}

# pass the null_system to systemfit
null_fit <- systemfit::systemfit(formula = null_formulas, data = df, method = "SUR")

# calculate the LR test statistic
lrtest_obj <- lmtest::lrtest(null_fit, sur_fit)
lr_statistic <- lrtest_obj$Chisq[2]
lr_df <- lrtest_obj$Df[2]

p_value <- stats::pchisq(lr_statistic, df = lr_df, lower.tail = FALSE)
print(sprintf("LR test statistic: %f, p-value: %f", lr_statistic, p_value))
```
The null hypothesis is that the coefficients of \texttt{treat} are zero in all the equations of the system. The alternative hypothesis is that there is at least one equation with a nonzero coefficient, implying that the control and treatment groups have different means for an OPV. 

The p-value of this test is significant (p-value < 0.05). This result shows that we can reject null hypothesis that the coefficients of \texttt{treat} are zero in all the equations of the system, implying that the assignment of treatment did not balance all the subjects' characteristics. 

<!-- TODO: How to verify manually?-->

3. Test that the OPVs do not predict treatment assignment.  Spell out null and alternative hypotheses, comment on findings, and verify the test's value and p-value ``manually.'' Why would scientists carry out this test? \textcolor{gray}{\textbf{Hint:} Use a MLRM and the \textcolor{ForestGreen}{F-test for the overall significance of the regression}. \textbf{Programming guidance}: Use \texttt{summary()} after estimation, e.g., \texttt{summary(lm\_fit)\$fstatistic} returns the test's value and degrees of freedom. Use \href{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Fdist.html}{\texttt{stats::pf()}} to verify the test's p-value.}

    We want to test that, given the covariates, the treatment assignment is random. We can do this by testing that the coefficients of the covariates are zero in the regression of treatment assignment on the covariates. \newline

    Given $(Y, X^{OPV})$ where $Y$ represents the treatment assignment, and $X$ is a vector of our OPV covariates, we can test the following model: $Y = \beta'X^{OPV} + \epsilon$ with these hypotheses: \newline 
    \[ 
    \begin{cases}
    H_0: \beta = 0 \\
    H_1: \beta \neq 0 
    \end{cases}
    \]
    In which $\beta$ is a vector of coefficients for the OPV covariates. Thus, we want $\beta = 0$, i.e the OPV covariates all have no correlation to the treatment $Y$.
    
```{r}
# create a list of formulas for each equation

lm_fit <- lm(formula = formula(paste("treat", "~", paste(vars, collapse = "+"))), data = df)
# summary(lm_fit)
# summary(lm_fit)$fstatistic
f_stat <- summary(lm_fit)$fstatistic[1]
numdf <- summary(lm_fit)$fstatistic[2]
dendf <- summary(lm_fit)$fstatistic[3]
p_val <- stats::pf(f_stat,numdf,dendf)

sprintf("The p-value of the ftest is: %f", p_val)

```

<!--
    Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  8.703e-01  2.586e-01   3.366 0.000831 ***
age          1.414e-03  3.412e-03   0.415 0.678704    
edu         -2.320e-02  1.704e-02  -1.361 0.174072    
black       -3.351e-02  8.758e-02  -0.383 0.702221    
hisp        -1.929e-01  1.160e-01  -1.663 0.097042 .  
married      4.703e-02  6.558e-02   0.717 0.473722    
nodegree    -2.126e-01  7.411e-02  -2.869 0.004320 ** 
re74        -1.090e-05  6.527e-06  -1.671 0.095539 .  
re75         6.143e-06  1.117e-05   0.550 0.582730    
re78         9.242e-06  3.547e-06   2.606 0.009480 ** 
u74         -5.498e-02  8.845e-02  -0.622 0.534513    
u75         -6.919e-02  7.625e-02  -0.907 0.364724    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4847 on 433 degrees of freedom
Multiple R-squared:  0.05892,   Adjusted R-squared:  0.03501 
F-statistic: 2.464 on 11 and 433 DF,  p-value: 0.005344

summary(lm_fit)$fstatistic
> summary(lm_fit)$fstatistic
     value      numdf      dendf 
  2.464357  11.000000 433.000000 

  -->

<!-- TODO: Intepret these results -->

With a p-value of 0.994656, we conclude that there is not enough evidence to reject the null hypothesis that OPVs do not predict treatment assignment. This result implies that proper random assignment was carried out such that treatment was given randomly irrespective of the subjects' characteristics. Scientists carry out this test to ensure that the treatment and control groups are similar in all respects except for the treatment itself.

\textbf{Part 3: Estimate the Effect of the Offer of Training}

1. Implement specs 0 (3 p) and 1 (2 p). Describe your findings.

```{r}
stats::t.test(df1$re78, df2$re78)
```

```{r}
spec1 = stats::lm(data=df, re78 ~ treat)
summary(spec1)
```


2. Implement specs 2 through 4. 

a. Report the estimates of the ATE and test $H_0$ that ATE is zero (3 p each spec). \textcolor{gray}{\textbf{Hint:} You consider spec 2 in light of the imbalance in educational attainment documented in Part 2. This approach to account for the presence of observable confounders (i.e., imbalance in OPVs) is called the \textcolor{ForestGreen}{regression adjustment approach}. Spec 3 has 11 regression covariates. Spec 4 has 12 regression covariates.}

```{r}
spec2 = stats::lm(data=df, re78 ~ treat + factor(nodegree) + edu)
summary(spec2)
```

```{r}
spec3 = stats::lm(data=df, re78 ~ treat + factor(nodegree) + edu + age + factor(black) + factor(hisp) + factor(married) + re74 + re75 + factor(u74) + factor(u75))
summary(spec3)
```

```{r}
df$age_dev = (df$age - mean(df$age))/sd(df$age)
```

```{r}
spec4 = stats::lm(data=df, re78 ~ treat + factor(nodegree) + edu + age + factor(black) + factor(hisp) + factor(married) + re74 + re75 + factor(u74) + factor(u75) + age_dev * treat)

summary(spec4)
```


b. Are there reasons to add OPVs as covariates when they are balanced across treatment and control groups? If so, what are they? Comment on the estimation results from specs 2 and 3.

We can add additional OPVs even if they're balanced across the treatment and control groups because even with no systematic differences between the groups, those variables may still possibly contribute some causal effect on the outcome. By regressing on those variables, we can remove some variance in the treatment effect that may be attributed to those OPVs. Additionally, even in randomized experiments, some imbalance can occur purely by chance. In spec 2, the p-value is not as significant as the p-value in spec 3. 

c. With reference to spec 3, do you think that it is problematic to use lagged / past values of the dependent variable as regression covariates? Explain.

No. It is common practice in time series analysis to regress on lagged values of the dependent variable. Though some assumptions must be upheld. In this case specifically, it seems alright as the past values of earnings show no significant effect on earnings in 1978 as a result of the t-test.

d. Are there reasons to add interactions between OPVs and the treatment indicator? If so, what are they? With reference to spec 4, test the following two hypothesis: i) the ATE is zero; ii) the effect of the treatment does not vary by the age of the subject. \textbf{Note:} If additional assumptions are needed to carry out i) and ii), state them. \textcolor{gray}{\textbf{Programming Guidance}: Use \href{https://rdrr.io/cran/car/man/linearHypothesis.html}{\texttt{car::linearHypothesis()}}.}

3. Brainstorm on the possible mechanisms for the estimated ATE of being offered training. \textcolor{gray}{\textbf{Hint:} Can you think of the possible pathways through which on-the-job training may cause an increase in post-intervention earnings?}\label{item:oot:ate-vs-ite}

A potential mechanism for the estimated ATE of being offered training could be that if an individual who if offered training accepts and undergoes the training, they may received skill enhancement which will enable them to secure better-paying positions that have more skill requirements. Another possible mechanism for the estimated ATE of being offered training is suggested by the footnote, that an individual might become more optimistic about their prospects and therefore works harder to land a well-paying job, even without actually going through the training.

4. Starting with Hint 1, we express $(y_{1i}^o, y_{0i}^o)$ in terms of $(y_{1i}, y_{0i})$.
First, recognize that individuals that are not offered training, cannot take up training. 
This means that the earnings of individuals not offered training, are a subset of the group of individuals that do not take up training. 
We can assert that $y_{0i}^o = y_{0i}$ by recognizing this relationship.\par\medskip
Then, recognize that individuals that are offered training, have a 0.5 probability of accepting or rejecting the offer of training. 
Individuals who accept the offer compose the group of individuals that recieve training. 
Individuals who reject the offer are part of the a subset of the group of individuals that do not take up training. 
This can be expressed as 
\begin{align*}
    y_{1i}^o &= \mathbb{P}[\text{accept offer}]y_{1i} + \mathbb{P}[\text{reject offer}]y_{0i}\\ 
    &= \frac{1}{2}y_{1i} + \frac{1}{2}y_{0i}.
\end{align*}
We then find the following relationship between $ATE$ and $ATE^o$: 
\begin{align*}
    ATE^o &= \mathbb{E}[y_{1i}^o - y_{0i}^o]\\
    &= \mathbb{E}[(\frac{1}{2}y_{1i} + \frac{1}{2}y_{0i}) - y_{0i}] \text{, by definition, }\\
    &= \mathbb{E}[\frac{1}{2}y_{1i} - \frac{1}{2}y_{0i}]\\
    &= \frac{1}{2}\mathbb{E}[y_{1i} - y_{0i}] \text{, by linearity, }\\
    &= \frac{1}{2}ATE.
\end{align*}
Using this model we have shown that the two average treatment effects are different.
Particularly that the Intent to Treat effect is of captured as a proportion of the Average Treatement Effect of recieving training. 



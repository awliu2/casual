% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{amsmath}
\usepackage{optidef}
\usepackage{accents}
\usepackage{caption}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Problem Set 6: Estimation of TEs Using Matching Methods},
  pdfauthor={Tessie Dong, Derek Li, Andi Liu},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Problem Set 6: Estimation of TEs Using Matching Methods}
\author{Tessie Dong, Derek Li, Andi Liu}
\date{Due Feb 15th, 2024}

\begin{document}
\maketitle

\input{figures/premise.tex}

\newpage

\subsection{Part 1: Naive Stratification Matching on the Propensity
Score (50
Points)}\label{part-1-naive-stratification-matching-on-the-propensity-score-50-points}

\input{figures/part1_background.tex}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (5 p) Let \(\widehat{p}_i\) denote the Logit-based estimate of the
  pscore for unit \(i\) which you obtained in PSet 5. Drop all sample
  units whose \(\widehat{p}_{i}\) falls outside of the common support.
  Why may a researcher want to trim the sample in this way? How many
  control (treated) units do you drop? Are you surprised? In all
  subsequent questions you use the resulting
  \textcolor{ForestGreen}{trimmed sample}.
  \textcolor{gray}{\textbf{Programming Guidance:} Use \texttt{dplyr::filter()} to drop rows from a dataframe. Save the resulting dataframe as a new object; in our R script it is called \texttt{psid\_trimmed}.}\label{item:pscore-trim}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(filename)\{}
\NormalTok{  dt }\OtherTok{\textless{}{-}}\NormalTok{ data.table}\SpecialCharTok{::}\FunctionTok{as.data.table}\NormalTok{(}
\NormalTok{    readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(filename,}
                     \AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{,}
                     \AttributeTok{col\_types =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{cols}\NormalTok{(}
                       \AttributeTok{treat =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{age =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{edu =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{black =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{hisp =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{married =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{re74 =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_double}\NormalTok{(),}
                       \AttributeTok{re75 =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_double}\NormalTok{(),}
                       \AttributeTok{re78 =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_double}\NormalTok{(),}
                       \AttributeTok{u74 =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{u75 =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{(),}
                       \AttributeTok{nodegree =}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{col\_integer}\NormalTok{()}
\NormalTok{                     ))}
\NormalTok{  )}
  \FunctionTok{return}\NormalTok{(dt)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{filename\_psid }\OtherTok{=} \StringTok{"starter{-}files/nswpsid.csv"}

\NormalTok{dt\_psid }\OtherTok{\textless{}{-}} \FunctionTok{load\_data}\NormalTok{(}\AttributeTok{filename =}\NormalTok{ filename\_psid)}

\NormalTok{dt\_psid[ , }\StringTok{\textasciigrave{}}\AttributeTok{:=}\StringTok{\textasciigrave{}}\NormalTok{ (}\AttributeTok{agesq =}\NormalTok{ age}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{edusq =}\NormalTok{ edu}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }
                 \AttributeTok{re74sq =}\NormalTok{ re74}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{re75sq =}\NormalTok{ re75}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{u74black=}\NormalTok{u74}\SpecialCharTok{*}\NormalTok{black)]}

\NormalTok{pscore\_formula }\OtherTok{=} \FunctionTok{as.formula}\NormalTok{(treat }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ agesq }\SpecialCharTok{+}\NormalTok{ edu }\SpecialCharTok{+}\NormalTok{ edusq }\SpecialCharTok{+} 
\NormalTok{                              married }\SpecialCharTok{+}\NormalTok{ nodegree }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ hisp }\SpecialCharTok{+} 
\NormalTok{                              re74 }\SpecialCharTok{+}\NormalTok{ re75 }\SpecialCharTok{+}\NormalTok{ re74sq }\SpecialCharTok{+}\NormalTok{ re75sq }\SpecialCharTok{+}\NormalTok{ u74black)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logit\_obj }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{glm}\NormalTok{(pscore\_formula, }\AttributeTok{family=}\FunctionTok{binomial}\NormalTok{(), }\AttributeTok{data =}\NormalTok{ dt\_psid)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt\_psid}\SpecialCharTok{$}\NormalTok{logit\_propensity }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit\_obj, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{control }\OtherTok{\textless{}{-}}\NormalTok{ dt\_psid }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{treatment }\OtherTok{\textless{}{-}}\NormalTok{ dt\_psid }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_cl }\OtherTok{=} \FunctionTok{min}\NormalTok{(control}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\NormalTok{p\_cu }\OtherTok{=} \FunctionTok{max}\NormalTok{(control}\SpecialCharTok{$}\NormalTok{logit\_propensity)}

\NormalTok{p\_tl }\OtherTok{=} \FunctionTok{min}\NormalTok{(treatment}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\NormalTok{p\_tu }\OtherTok{=} \FunctionTok{max}\NormalTok{(treatment}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trimmed\_df }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(dt\_psid, logit\_propensity }\SpecialCharTok{\textgreater{}=} 
              \FunctionTok{max}\NormalTok{(p\_cl, p\_tl) }\SpecialCharTok{\&}\NormalTok{ logit\_propensity }\SpecialCharTok{\textless{}=} \FunctionTok{min}\NormalTok{(p\_cu, p\_tu))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(control}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2490
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}} \FunctionTok{length}\NormalTok{((trimmed\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{))}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -1146
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(treatment}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 185
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}} \FunctionTok{length}\NormalTok{((trimmed\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{))}\SpecialCharTok{$}\NormalTok{logit\_propensity)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -179
\end{verbatim}

\textbf{Solution:} A researcher may want to trim the sample in this way
because if we are interested in matching p-scores between treatment and
control groups, dropping p-scores that are too low or too high allows
the OPVs between the groups to be more balanced. This is because we are
removing units with certain covariates that are unlikely or very likely
to be treated, in order to avoid cases in which there are no good
matches for a unit from the treatment group within the control group (or
vice versa).

We dropped 1344 control units and 6 treated units. This is not
surprising given the difference in the bounds, where \(\hat{p}^{T,l}\)
is almost \(10e6\) times as large compared to \(\hat{p}^{C,l}\) whilst
the upper bounds are within \(0.001\) of one another. However, a
significant portion of the dropped control units have p scores that are
very close to zero - i.e they would likely not be matched to a treated
unit.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  (5 p) Figure \ref{fig:propensity_vs_earnings} plots \texttt{re78} (the
  outcome metric) against \(\widehat{p}_{i}\), separately for the
  treated and the control groups. Each panel also includes a curve that
  displays the fitted non-parametric regression of \texttt{re78} on
  \(\widehat{p}_{i}\) i.e., a smoothed estimate of the mean of earnings
  conditional on the value of the pscore. Take for example the curve in
  the Treated panel, it connects earnings values constructed as follows.
  Pick a value of the pscore, e.g., \(p=0.89567\). Identify the (for
  example) 10 closest pscore values in the treated sample. Using these
  11 points, regress \texttt{re78} on a constant and the pscore values,
  giving more weight to the pscore values closest to \(p=0.89567\). The
  implied fitted value of earnings at \(p=0.89567\) is the value
  depicted on the curve.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Reproduce Figure \ref{fig:propensity_vs_earnings}. Test your
  understanding of the Loess smoothing method by varying the value of
  \texttt{span}.
  \textcolor{gray}{\textbf{Programming Guidance:} Use the script below which assumes that the trimmed dataframe is \texttt{trimmed\_df} and the Logit-based estimates are in column \texttt{pscore}. \texttt{ggplot2::ggplot()} produces a scatter plot. \href{https://ggplot2.tidyverse.org/reference/geom_smooth.html}{\texttt{geom\_smooth}}\texttt{(method = ``loess'', span = 0.5, method.args = list(degree = 1))} overlays a smoothed curve: (i) the \texttt{method} argument picks the LOWESS (LOcally WEighted Scatter-plot Smoother) smoothing method, i.e., the R function \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess}{\texttt{stats::loess()}}; (ii) the \texttt{span} argument indicates the proportion of the total number of points that contribute to each local fitted value; (iii) the \texttt{method.args} argument lists additional arguments passed on to the function \texttt{stats::loess()}, in particular \texttt{degree = 1} picks a polynomials of degree 1 (which is just a line). Here is a \href{https://ggplot2.tidyverse.org/reference/geom_smooth.html}{link} to some examples.}
\end{enumerate}

\textbf{Discussion of span:} Using the provided R script, we find that
in general, decreasing the value of \texttt{span} results in a more
over-fit line, and increasing its value results in a smoother line. This
lines up with documentation regarding the \texttt{span} parameter: it is
the proportion of total number of points that contribute to each local
fitted value, and thus, using less points for each fitted value leads to
over fitting, and using more points leads to a smoother line. We provide
two plots for comparison: the first with \texttt{span\ =\ 0.5}, and the
second with \texttt{span\ =\ 0.25}.

\includegraphics{pset_6_files/figure-latex/unnamed-chunk-10-1.pdf}

\includegraphics{pset_6_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  (3 p) Can you eyeball estimate\textbf{s} of the TE of the offer of
  training from these figures? Explain.\\
  \textbf{Solution:} From the two smoothed cruves on the graphs, we can
  roughly eyeball that the TEs are not homogenous across different
  pscores. For example, we eyeball that for pscores \textless{} 0.5, the
  estimates of the TE are roughly 0 as there does not seem to be a
  significant difference between the real earnings in '78 for the
  control and treated groups. However, for pscores that are
  \textgreater{} 0.5, we eyeball that the estimates of TE are positive
  (e.g.~for pscore = 0.75, the TE seems to be \textasciitilde1.5k),
  implying that there is a positive impact of the training offer on real
  earnings in '78 for those pscores.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  (40 p) We can do better than eyeballing TEs. You are now ready to
  implement \textcolor{ForestGreen}{stratification matching}.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  (5 p) Add a column to your dataframe called \texttt{stratum} that
  takes values from 1 to 10 corresponding to 10 equally spaced intervals
  with \texttt{stratum=1} if \(0<\widehat{p}_{i}\leq 0.1\),
  \texttt{stratum=2} if \(0.1<\widehat{p}_{i}\leq 0.2\), and so
  on.\footnote{\textit{Strata} is the plural of \textit{stratum}, both are Latin words.}
  What is the distribution of control and treated observation across
  strata? Are all strata \textcolor{ForestGreen}{populated}?
  \textcolor{gray}{\textbf{Programming Guidance:} To bin the values of a column, use the R base function \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/.bincode( )}{\texttt{.bincode}} with break points at 0, 0.1, 0.2 etc. up to 1. To add a column to a dataframe, use \texttt{dplyr::mutate( )}. To produce summary statistics by group, use \texttt{dplyr::group\_by( )}, and pipe the result to \texttt{dplyr::summarize( )}. In our R script we produce the following summary statistics within each stratum: count of treated units, count of control units, minimum pscore value, maximum pscore value.}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{trimmed\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(trimmed\_df, }
                            \AttributeTok{stratum =} \FunctionTok{.bincode}\NormalTok{(trimmed\_df}\SpecialCharTok{$}\NormalTok{logit\_propensity, b))}

\NormalTok{strata }\OtherTok{\textless{}{-}}\NormalTok{ trimmed\_df }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(trimmed\_df}\SpecialCharTok{$}\NormalTok{stratum) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarize}\NormalTok{(}\AttributeTok{count\_of\_treated =} \FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)), }
                  \AttributeTok{count\_of\_control =} \FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)), }
                  \AttributeTok{proportion\_treated =} \FunctionTok{mean}\NormalTok{(treat))}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(strata)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2639}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2361}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2361}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2639}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
trimmed\_df\$stratum
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
count\_of\_treated
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
count\_of\_control
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
proportion\_treated
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 11 & 1018 & 0.0106900 \\
2 & 7 & 53 & 0.1166667 \\
3 & 11 & 24 & 0.3142857 \\
4 & 16 & 17 & 0.4848485 \\
5 & 5 & 8 & 0.3846154 \\
6 & 15 & 6 & 0.7142857 \\
7 & 14 & 8 & 0.6363636 \\
8 & 8 & 5 & 0.6153846 \\
9 & 13 & 0 & 1.0000000 \\
10 & 79 & 7 & 0.9186047 \\
\end{longtable}

\textbf{Discussion:} The distribution of control and treated observation
across strata is shown in the table above. We see that within stratum 9
(between 0.8 and 0.9), there are no control units. Besides that stratum,
all others are populated by both treated and control units, albeit with
different distributions. We also observe that there is a general trend
of increasing proportion of treated units as we move from stratum 1 to
stratum 10.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  (25 p) Test that the average of each pretreatment variable is the same
  for treated and control units within each stratum: the pre-treatment
  variables are
  \texttt{age, edu, married, nodegree, black, hisp, re74, re75, u74black}.
  Can you think of a reason why you may want to run this test? Comment
  on your findings.
  \textcolor{gray}{\textbf{Programming Guidance:} Use SUR estimation as you did in PSet 3, but now do this within each stratum. Skip strata that do not have both control and treated units. Within some of the strata, some of the pre-treatment variables are perfectly collinear or do not vary. This causes \texttt{systemfit::systemfit( )} to produce an error. To avoid it, you first identify which of the pre-determined variables exhibit the problem, then you exclude them from the list of dependent variables of the SUR system, then you invoke \texttt{systemfit::systemfit( )}. To loop over strata, use the \texttt{for} loop: \texttt{for(stratrum in 1:10) \{ $<$ script $>$\}}. Below is a user-defined function that returns the names of the collinear columns in a dataframe, feel free to use it:}
\end{enumerate}

\textbf{Solution:} We may want to run this test to ensure that the
pretreatment variables are balanced across the treated and control
groups within each stratum. This is important because even though it may
be the case that the pre-treatment variables are balanced across the
entire dataset, we want to ensure that they are still balanced even
after we stratify by pscore. Since we are estimating ATT using
difference in post-treatment earnings within each of our stratum, we
would like that OPV balance still holds within each of these sub-groups
of data. Per the results below, we see that most of the stratum (that
have both treated and control individuals) have balanced OPVs at a 0.05
level. However, we see that strata 5 and 8 in particular have p-values
less than 0.05, indicating that at least one of the OPVs is
significantly different across the treatment/control groups and we
cannot be confident that the OPVs are balanced within these strata. We
might consider that the small sample sizes of these strata may be
causing the imbalance in OPVs, and perhaps we could consider dropping
these strata, or combining them with other strata to ensure that we have
balanced OPVs within each stratum.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Declare a function that identifies perfectly collinear covariates.}
\CommentTok{\# Note: this fnc is roughly equivalent to \_rmcoll in STATA.}
\NormalTok{rmcoll }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(df, }\AttributeTok{colnames =} \FunctionTok{names}\NormalTok{(df)) \{}
  \CommentTok{\# Arguments:}
  \CommentTok{\# {-} df: A data frame.}
  \CommentTok{\# {-} colnames: A list of column names.}
  \CommentTok{\# Returns:}
  \CommentTok{\# A list with the column names of collinear variables.}
\NormalTok{    df\_ }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{one\_of}\NormalTok{(colnames))}
\NormalTok{    cc }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(}\FunctionTok{lm}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(df\_)) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ df\_))}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{names}\NormalTok{(cc)[}\FunctionTok{is.na}\NormalTok{(cc)])}
\NormalTok{\} }\CommentTok{\# end fnc rmcoll}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stratum\_results }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{opvs }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"edu"}\NormalTok{, }\StringTok{"married"}\NormalTok{, }\StringTok{"nodegree"}\NormalTok{, }\StringTok{"black"}\NormalTok{,}
            \StringTok{"hisp"}\NormalTok{, }\StringTok{"re74"}\NormalTok{, }\StringTok{"re75"}\NormalTok{, }\StringTok{"u74black"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{(stratum\_i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:} \DecValTok{10}\NormalTok{)\{}
\NormalTok{  subset }\OtherTok{\textless{}{-}}\NormalTok{ trimmed\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(stratum }\SpecialCharTok{==}\NormalTok{ stratum\_i)}
  \CommentTok{\# Skip strata that do not have either no control or no treated units}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{sum}\NormalTok{(subset}\SpecialCharTok{$}\NormalTok{treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\textless{}=} \DecValTok{0} \SpecialCharTok{||} 
      \FunctionTok{sum}\NormalTok{(subset}\SpecialCharTok{$}\NormalTok{treat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{\textless{}=} \DecValTok{0}\NormalTok{) \{}
    \ControlFlowTok{next}
\NormalTok{  \}}
  
\NormalTok{  non\_collinear }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(opvs, }\FunctionTok{rmcoll}\NormalTok{(subset, opvs))}

\NormalTok{  sur\_system }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{  null\_system }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
  \ControlFlowTok{for}\NormalTok{ (v }\ControlFlowTok{in}\NormalTok{ non\_collinear) \{}
\NormalTok{    sur\_system[[v]] }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(v, }\StringTok{"\textasciitilde{} treat"}\NormalTok{))}
\NormalTok{    null\_system[[v]] }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(v, }\StringTok{"\textasciitilde{} 1"}\NormalTok{))}
\NormalTok{  \}}
  \CommentTok{\# null\_system \textless{}{-} map(vars, function(x) formula(paste(x, "\textasciitilde{} 1")))}
\NormalTok{  sur\_fit }\OtherTok{\textless{}{-}}\NormalTok{ systemfit}\SpecialCharTok{::}\FunctionTok{systemfit}\NormalTok{(}\AttributeTok{formula=}\NormalTok{sur\_system, }\AttributeTok{data=}\NormalTok{subset, }\AttributeTok{method=}\StringTok{"SUR"}\NormalTok{)}
\NormalTok{  null\_fit }\OtherTok{\textless{}{-}}\NormalTok{ systemfit}\SpecialCharTok{::}\FunctionTok{systemfit}\NormalTok{(}\AttributeTok{formula=}\NormalTok{null\_system, }\AttributeTok{data=}\NormalTok{subset, }\AttributeTok{method=}\StringTok{"SUR"}\NormalTok{)}

\NormalTok{  lrtest\_obj }\OtherTok{\textless{}{-}} \FunctionTok{lrtest}\NormalTok{(null\_fit, sur\_fit)}

\NormalTok{  row }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{stratum =}\NormalTok{ stratum\_i, }\AttributeTok{p\_value =}\NormalTok{ lrtest\_obj}\SpecialCharTok{$}\StringTok{\textquotesingle{}Pr(\textgreater{}Chisq)\textquotesingle{}}\NormalTok{[}\DecValTok{2}\NormalTok{])}
\NormalTok{  stratum\_results }\OtherTok{\textless{}{-}} \FunctionTok{append}\NormalTok{(stratum\_results, }\FunctionTok{list}\NormalTok{(row))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}
\caption{P-values of the test of the equality of the OPVs across the
treated and control groups within each stratum.}\tabularnewline
\toprule\noalign{}
stratum & p\_value \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
stratum & p\_value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.7031115 \\
2 & 0.3172816 \\
3 & 0.8192198 \\
4 & 0.9730491 \\
5 & 0.0000000 \\
6 & 0.5831040 \\
7 & 0.2434995 \\
8 & 0.0352425 \\
10 & 0.9794200 \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\item
  (10 p) Let \(N_{s}^{T}\) denote the number of treated units in stratum
  \(s\) with \(s=1,...,10\). Let \(N^{T}=\sum_{s=1}^{10}N_{s}^{T}\). Let
  \(\overline{re78}_{s}^{1}\) (respectively,
  \(\overline{re78}_{s}^{0}\)) denote average post-treatment earnings
  for treated (respectively, control) units in stratum \(s\). Estimate
  the ATT of the offer of training using stratification matching
  estimator (\ref{strata_ATT}). Explain in plain English why this is an
  estimator of the ATT.
  \textcolor{gray}{\textbf{Programming Guidance:} To compute a weighted average use \href{https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/weighted.mean}{\texttt{stats::weighted.mean( )}} where you let the argument \texttt{w} be a vector with the counts of treated units in each stratum. Verify that you get \$1,563.}\label{item:naive-ATT-estimate}

  \begin{equation}
   \widehat{ATT}^{SM}=\sum_{s=1}^{10}\left( \frac{N_{s}^{T}}{N^{T}}\right) \left( \overline{re78}_{s}^{1}-\overline{re78}_{s}^{0}\right) 
   \text{.}\label{strata_ATT}
   \end{equation}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{te }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\NormalTok{w }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:} \DecValTok{10}\NormalTok{) \{}
  
  \ControlFlowTok{if}\NormalTok{(i }\SpecialCharTok{==} \DecValTok{9}\NormalTok{)\{}
    \ControlFlowTok{next}
\NormalTok{  \}}
  
\NormalTok{  stratum\_df }\OtherTok{\textless{}{-}}\NormalTok{ trimmed\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(stratum }\SpecialCharTok{==}\NormalTok{ i)}
\NormalTok{  stratum\_treat }\OtherTok{\textless{}{-}}\NormalTok{ stratum\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{  stratum\_control }\OtherTok{\textless{}{-}}\NormalTok{ stratum\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{  re78\_1bar }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(stratum\_treat}\SpecialCharTok{$}\NormalTok{re78)}
\NormalTok{  re78\_0bar }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(stratum\_control}\SpecialCharTok{$}\NormalTok{re78)}
\NormalTok{  stratum\_te }\OtherTok{\textless{}{-}}\NormalTok{ re78\_1bar }\SpecialCharTok{{-}}\NormalTok{ re78\_0bar}
  
\NormalTok{  te }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(te, stratum\_te)}
  
\NormalTok{  weight }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(stratum\_treat)}
\NormalTok{  w }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(w, weight)}
\NormalTok{\}}

\NormalTok{att\_sm }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{weighted.mean}\NormalTok{(te, w}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(w)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\NormalTok{att\_sm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1562.73
\end{verbatim}

\textbf{Solution:} From above, we see that our calculated estimate of
the ATT to be \$1562.73 which rounds to \$1563, as desired. This is an
estimate of the ATT because we are computing the treatment effect within
the sub-population of the treated individuals. We can think of each
calculate TE within stratum as a CATE (conditional upon the stratum),
and then we performed a weighted sum of these CATEs (weighed by
proportion of treated within each stratum/total treated in the sample),
thus our final estimator is an estimator of the ATT.

\subsection{Part 2: Fancier Matching On the PScore (50
Points)}\label{part-2-fancier-matching-on-the-pscore-50-points}

\input{figures/part2_background.tex}
\newpage

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  (6 p) Denote the outcome variable by \(y\). Show that under
  unconditional random assignment (RA),
  \(\widehat{ATT}^{m}=\overline{y}^{1}-\overline{y}^{0},\)
  \(\forall m\in \left\{ NNM,RM,KM\right\}\). That is, if there are no
  observable confounders, all three estimators reduce to the
  Treatment-Control Comparison estimator.
  \textcolor{gray}{\textbf{Hint:} Assign treatment by the flip of a balanced coin, i.e., let $\widehat{p}_{i}=0.5$ for all $i$. Then, simplify expressions (\ref{ATT_m}) and (\ref{w_ij}). Do read carefully the background information presented above.}
\end{enumerate}

\textbf{Solution:} First leverage that \(\hat{p}_i = 0.5 \ \forall i\)
and (2) to show that \begin{align*}
  C^{NNM}(i) &= \{j \in C : |\hat{p}_j - \hat{p}_i| \leq |\hat{p}_{j'} - \hat{p}_i| \ \forall j' \in C\}\\
  &= \{j \in C : |0.5 - 0.5| \leq |0.5 - 0.5| \}\\
  &= \{j \in C : 0 \leq 0 \}\\ 
  &= \{j \in C : True \}\\ 
  &= C 
\end{align*}

Then recognize that with the same assumption in (3), we show that
\begin{align*}
  C^{RM}(i) &= \{j \in C : |\hat{p}_j - \hat{p}_i| < r\}\\
  &= \{j \in C : |0.5 - 0.5| < r \}\\
  &= \{j \in C : 0 < r \}\\ 
  &= C \text{, given that we choose $r > 0$}. 
\end{align*}

We have from (\ref{ATT_m}) and (\ref{w_ij}) for \(m \in \{NNM, RM\}\)
that:

\begin{align*}
    \widehat{ATT}^{m} &= \frac{1}{N^T}\sum_{i\in T} \left[y_i - \sum_{j \in C^{m}(i)} w_{ij}^m y_j\right]\\ 
                      &= \frac{1}{N^T}\sum_{i \in T} \left[  y_i - \sum_{j \in C} \frac{1}{N^C} y_j\right] \text{, by previous derivation showing that $C^{m}(i) = C$, }\\
                      &= \frac{1}{N^T}\sum_{i \in T}  \left[ y_i - \frac{1}{N^C} \sum_{j \in C\left( i\right)} y_j \right] \text{ from lin. of $\sum$}\\
                      &= \frac{1}{N^T}\sum_{i \in T}  \left[ y_i - \bar{y}^0 \right] \text{, by definition, }\\
                      &= \frac{1}{N^T}\sum_{i \in T} [y_i] - \frac{1}{N_T} \sum_{i\in T} [\bar{y}^0]\text{, by $\sum$ properties, }\\
                      &= \bar{y}^1 - \frac{1}{N^T} \cdot N^T \cdot \bar{y}^0 \text{, by summation over a constant, } \\
                      &= \bar{y}^1 - \bar{y}^0. 
\end{align*}

Now we show that this is also true for the KM estimator. We have from
(\ref{ATT_m}) and (\ref{w_ij}) that: \begin{align*}
    \widehat{ATT}^{KM} &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \sum_{j \in C^{KM}\left( i\right)}\frac{K\left( \frac{\hat{p}_{j} - \hat{p}_{i}}{h}\right)}{\sum_{k \in C}K \left( \frac{\hat{p}_{k}-\hat{p}_{i}}{h}\right)} y_j \right ] \\
  \end{align*}

Under RA, we can set probabilities
\(\hat{p_k}, \ \hat{p_i}, \ \hat{p_j}\) equal to 0.5 and simplify the
expression to:

\begin{align*}
    \widehat{ATT}^{KM} &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \sum_{j \in C^{KM}\left( i\right)}\frac{K\left(0\right)}{\sum_{k \in C} K \left(0\right)} y_j \right] \\
                       &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \sum_{j \in C^{KM}\left( i\right)}\frac{K\left(0\right)}{N^c \cdot K \left(0\right)} y_j \right] \text{ from summation over a constant,} \\
                       &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \frac{1}{N^c} \sum_{j \in C^{KM}\left( i\right)} y_j \right] \text{ via lin. of $\sum$,} \\
                       &= \frac{1}{N^T} \sum_{i \in T} \left[  y_i - \frac{1}{N^c} \sum_{j \in C} y_j \right] \\
                       &= \frac{1}{N^T} \left[ \sum_{i \in T} y_i - \bar{y}^0 \right] = \overline{y}^{1} - \overline{y}^{0}. & \blacksquare
  \end{align*}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  (9 p) Show that \(\widehat{ATT}^{m}\) in expression (\ref{ATT_m})
  rewrites as:
\end{enumerate}

\begin{equation} \label{ATT_m_bis}
\widehat{ATT}^{m} =\overline{y}^{1}-\overline{y}_{m}^{0} \ \forall m\in \left\{ NNM,RM,KM\right\},
\end{equation}

\noindent where:

\begin{equation}\label{ATT_m_bis_w}
\overline{y}^{1} =\frac{1}{N^{T}}\sum_{j \in T}y_{j}, \text{     and      }  \overline{y}_{m}^{0} =\frac{1}{N^{C}}\sum_{j \in C}\pi_{j}^{m}y_{j}.
\end{equation}

\noindent Use expression (\ref{ATT_m_bis}) to describe in plain English
the essence of each matching approach.
\textcolor{gray}{\textbf{Hint:} Find the expression for $\pi _{j}^{m}$ in expression (\ref{ATT_m_bis_w}) for each $m\in \left\{NNM,RM,KM\right\} $}.

\textbf{Solution:} Consider the following: \begin{align*}
 \hat{ATT}^m &= \frac{1}{N^T} \sum_{i\in T}\left[y_i - \sum_{j\in C^m(i)} w_{ij}^m y_j\right], \text{, by definition, }\\
 &= \frac{1}{N^T} \sum_{i\in T}[y_i] - \frac{1}{N^T} \sum_{i\in T}\sum_{j\in C^m(i)} w_{ij}^m y_j \text{, by properties of $\sum$, }\\
 &= \bar{y}^1 - \frac{1}{N^T} \sum_{i\in T}\left[\sum_{j\in C^m(i)} w_{ij}^m y_j\right] \text{, by definition. }
\end{align*}

Now let \(m \in \{NNM, RM\}\), for arbitrary treatment unit \(i\),
\begin{align*}
    \sum_{j\in C}w_{ij}^m y_j &= \sum_{j\in C^m(i)} w_{ij}^m y_j + \sum_{j\in C\setminus C^m(i)} 0 \cdot y_j \text{, by (6), }\\
    &= \sum_{j\in C^m(i)} w_{ij}^m y_j + 0 \\
    &= \sum_{j\in C^m(i)} w_{ij}^m y_j.  
\end{align*}

Similarly, for \(m = KN\) we have by definition (6) that \(C^m(i) = C\).
For all \(m\) we thus have,
\[\sum_{j\in C}w_{ij}^m y_j = \sum_{j\in C^m(i)} w_{ij}^m y_j.\]

We can continue to then show that \begin{align*}
    \hat{ATT}^m &= \bar{y}^1 - \frac{1}{N^T} \sum_{i\in T}\left[\sum_{j\in C^m(i)} w_{ij}^m y_j\right] \\ 
    &= \bar{y}^1 - \frac{1}{N^T} \sum_{i\in T}\left[\sum_{j\in C} w_{ij}^m y_j\right]\\ 
    &= \bar{y}^1 - \frac{1}{N^T} \sum_{j\in C} \left[ \left(\sum_{i\in T} w_{ij}^m \right) y_j \right] \text{, because $T,C$ are disjoint and $i,j$ are independent,  }\\ 
    &= \bar{y}^1 - \frac{1}{N^C} \cdot \frac{N^C}{N^T} \cdot \sum_{j\in C} \left[ \left(\sum_{i\in T} w_{ij}^m\right) y_j \right] \text{, by multiplying by 1, }\\
    &= \bar{y}^1 - \frac{1}{N^C} \sum_{j\in C} \left[ \left(\frac{N^C}{N^T} \sum_{i\in T} w_{ij}^m \right) y_j \right]\text{, by linearity of summation, }\\ 
    &= \bar{y}^1 - \frac{1}{N^C} \sum_{j\in C} \pi_{j}^m y_j \text{, where } \pi_j^m = \frac{N^C}{N^T} \sum_{i\in T} w_{ij}^m \text{,}\\
    &= \bar{y}^1 - \bar{y_m^0} \text{, by definition.}  
\end{align*}

In plain English we recognize that the matching approach is a difference
between the sample average of the treated sample and a weighted average
of the control sample. In particular each method uses a different
weighting mechanism by leveraging (2), (3), (4) and (6) together. For
nearest neighbors, we weight by a set of control units that have closest
distance in terms of propensity score. In radius matching, we choose
control units that are within certain neighborhood of propensity scores.
For kernel matching we choose a kernel function as the distance metric
and weight propensity by similarity.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  (3 p) With the trimmed sample from Part 1, produce the 1:1 NNM
  estimate of ATT using
  \href{https://www.rdocumentation.org/packages/Matching/versions/4.9-6/topics/Match}{\texttt{Matching::Match( )}}
  from the package
  \href{https://cran.r-project.org/web/packages/Matching/Matching.pdf}{\texttt{Matching}}.
  Verify that it coincides with that presented in Table
  \ref{tab:Tab25-4_2}.
  \textcolor{gray}{\textbf{Note:} Table \ref{tab:Tab25-4_2} also reports estimates based on 3 other PSM approaches (you are not asked to implement them). ``Adaptive Stratification'' is a refinement of the ``Naive Stratification'' approach that you implemented in Part 1: strata are created iteratively (as opposed to being fixed ahead of time) so as to ensure balance within each stratum.}
  \textcolor{gray}{\textbf{Programming guidance:} \texttt{Matching::Match()} takes 3 dataframes as inputs, \texttt{Y}, \texttt{Tr} and \texttt{X}. \texttt{Y} and \texttt{Tr} are columns \texttt{re78} and \texttt{treat} respectively in your dataframe, and \texttt{X} is the column that stores the Logit estimates of the pscore because you want to match on the estimated pscore. Specify \texttt{M=1} and \texttt{estimand=``ATT''} so that you estimate ATT using 1:1 matching. Use \texttt{summary()} on the object returned, which is a list of 23 elements.}
  \label{item:estimate-att-ncs}
\end{enumerate}

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\hline \hline
\textbf{Matching Estimator} & \textbf{Estimate (\$/year)} & \textbf{SE (\$/year)} \\ \hline
1:1 Nearest-Neighbor Matching with Replacement (NNM)       &  490.39 & 1,929.6  \\
Radius Matching with $r=0.0001$ (RM)  &  -5,546.139 &  4,614.773 \\
Gaussian Kernel Matching (KM)        &  1,537.947 & 861.329  \\ 
Adaptive Stratification Matching     &  2,208.603 &  855.168 \\ \hline
\end{tabular}
\caption{Estimates of ATT based on four pscore-matching estimators.}
\label{tab:Tab25-4_2}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y }\OtherTok{\textless{}{-}}\NormalTok{ trimmed\_df}\SpecialCharTok{$}\NormalTok{re78}
\NormalTok{Tr }\OtherTok{\textless{}{-}}\NormalTok{ trimmed\_df}\SpecialCharTok{$}\NormalTok{treat}
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ trimmed\_df}\SpecialCharTok{$}\NormalTok{logit\_propensity}

\NormalTok{matched }\OtherTok{\textless{}{-}}\NormalTok{ Matching}\SpecialCharTok{::}\FunctionTok{Match}\NormalTok{(Y,Tr,X,}\AttributeTok{M=}\DecValTok{1}\NormalTok{,}\AttributeTok{estimand=}\StringTok{"ATT"}\NormalTok{)}

\NormalTok{nnm\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(matched[}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}est\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}se\textquotesingle{}}\NormalTok{)])}
\FunctionTok{colnames}\NormalTok{(nnm\_df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}NNM Estimate\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}NNM Std. Error\textquotesingle{}}\NormalTok{)}

\NormalTok{summary\_matched }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(matched)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Estimate...  490.39 
## AI SE......  1929.6 
## T-stat.....  0.25414 
## p.val......  0.79939 
## 
## Original number of observations..............  1325 
## Original number of treated obs...............  179 
## Matched number of observations...............  179 
## Matched number of observations  (unweighted).  700
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(nnm\_df)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}
\toprule\noalign{}
NNM Estimate & NNM Std. Error \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
490.3947 & 1929.634 \\
\end{longtable}

From these outputs, we can verify that our results match exactly those
provided.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  (16 p) Consider the \textcolor{ForestGreen}{matched data}, i.e., the
  collection of pairs such that each pair is a treated unit and a
  matched control.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  (2 p) How many treated units are paired with at least one control
  unit?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_treated\_paired }\OtherTok{=} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(matched}\SpecialCharTok{$}\NormalTok{index.treated))}
\NormalTok{num\_treated\_paired}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 179
\end{verbatim}

  \textbf{Solution:} 179 treated units are paired with at least one
  control unit.
\item
  (2 p) How many treated units are paired with exactly one control unit?
  \newline \textbf{Solution:} 151 treated units are paired with at
  exactly one control unit, as seen below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{table}\NormalTok{(matched}\SpecialCharTok{$}\NormalTok{index.treated)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Freq }\SpecialCharTok{==} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 151
\end{verbatim}
\item
  (2 p) What percentage of control units serve as a match to at least
  one treated unit? \newline \textbf{Solution:} About 46.68\% of control
  units serve as a match to at least one treated unit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_control\_matched }\OtherTok{=} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(matched}\SpecialCharTok{$}\NormalTok{index.control))}
\NormalTok{total\_control\_units }\OtherTok{=} \FunctionTok{sum}\NormalTok{(Tr }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{control\_matched }\OtherTok{=}\NormalTok{ (num\_control\_matched }\SpecialCharTok{/}\NormalTok{ total\_control\_units) }\SpecialCharTok{*} \DecValTok{100}
\NormalTok{control\_matched}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 46.68412
\end{verbatim}
\item
  (2 p) Take the treated units that are paired with exactly 1 control
  unit, how many distinct control units serve as matches to these
  treated units? Are you surprised? \newline \textbf{Solution:} 535
  distinct control units serve as matches to these treated units.
  Initially, this was surprising because all the treated units are
  paired with exactly one control unit, but there are more matched
  control units than matched treated units.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_distinct\_controls\_for\_treated }\OtherTok{=} \FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(matched}\SpecialCharTok{$}\NormalTok{index.control))}
\NormalTok{num\_distinct\_controls\_for\_treated }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 535
\end{verbatim}
\item
  (2 p) The algorithm matches one treated unit with 209 distinct control
  units. How do you make sense of this? \newline \textbf{Solution:} This
  can happen because the many control units might have identical or very
  similar pscores relative to a treated unit. The matching algorithm
  handles ties by matching the treated unit to all equally good control
  matches (with the same pscore), this could result in a large number of
  matches for a single treated unit.
\item
  Make sense of the line
  ``\texttt{Matched number of observations (unweighted). 700}'\,' in
  Figure \ref{fig:nnm_output}.

  \begin{figure}[h]
   \centering
   \captionsetup{width=.6\textwidth}
   \includegraphics[width=0.4\textwidth]{figures/q7_f.png}
   \caption{Snapshot of output produced by running \texttt{symmary()} on the object returned by \texttt{Matching::Match()} in \textbf{\ref{item:estimate-att-ncs}}.}
   \label{fig:nnm_output}
   \end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste}\NormalTok{(}\StringTok{"length of index.treated:"}\NormalTok{, }\FunctionTok{length}\NormalTok{(matched}\SpecialCharTok{$}\NormalTok{index.treated), }
      \StringTok{"index.control:"}\NormalTok{, }\FunctionTok{length}\NormalTok{(matched}\SpecialCharTok{$}\NormalTok{index.control))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "length of index.treated: 700 index.control: 700"
\end{verbatim}

  \textbf{Solution:} This line means that there are 700 matched pairs.
  This does not take into account that one treated unit might be matched
  with multiple control units. We can obtain this result from the above
  code and verifying that both vectors have 700 elements.
\item
  (2 p) Use the matched data to ``manually'\,' compute the ATT estimate.
  \newline \textbf{Solution:} The manually computed ATT estimate is
  490.39.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(((trimmed\_df}\SpecialCharTok{$}\StringTok{\textquotesingle{}re78\textquotesingle{}}\NormalTok{)[matched}\SpecialCharTok{$}\NormalTok{index.treated] }\SpecialCharTok{{-}} 
\NormalTok{    (trimmed\_df}\SpecialCharTok{$}\StringTok{\textquotesingle{}re78\textquotesingle{}}\NormalTok{)[matched}\SpecialCharTok{$}\NormalTok{index.control]) }\SpecialCharTok{*}\NormalTok{ matched}\SpecialCharTok{$}\NormalTok{weights) }\SpecialCharTok{/} \DecValTok{179}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 490.3947
\end{verbatim}
\item
  (2 p) Run \texttt{Matching::Match()} with \texttt{ties = FALSE}. Why
  do you get
  ``\texttt{Matched number of observations (unweighted). 179}?''
  \newline \textbf{Solution:} I get ``Matched number of observations
  (unweighted). 179'' because setting ties = FALSE instructs the
  algorithm not to match treated units to more than one control unit
  when there are ties in the matching criterion (e.g.~distance based on
  propensity score). So we get exactly 179 matched pairs, i.e exactly
  the number of treated units in the sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matched\_notie }\OtherTok{\textless{}{-}}\NormalTok{ Matching}\SpecialCharTok{::}\FunctionTok{Match}\NormalTok{(Y,Tr,X,}\AttributeTok{M=}\DecValTok{1}\NormalTok{,}\AttributeTok{estimand=}\StringTok{"ATT"}\NormalTok{,}\AttributeTok{ties =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{(}\FunctionTok{summary}\NormalTok{(matched\_notie))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Estimate...  442.61 
## SE.........  769.62 
## T-stat.....  0.57511 
## p.val......  0.56522 
## 
## Original number of observations..............  1325 
## Original number of treated obs...............  179 
## Matched number of observations...............  179 
## Matched number of observations  (unweighted).  179
\end{verbatim}
\end{enumerate}

\newpage 
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{\texttt{num\_matches}} & \textbf{\texttt{num\_treated}} & \textbf{\texttt{min\_pscore}} & \textbf{\texttt{avg\_pscore}} & \textbf{\texttt{med\_pscore}} & \textbf{\texttt{max\_pscore}} \\ \hline
209 & 1 & 0.000652575 & 0.000652575 & 0.000652575 & 0.000652575 \\ \hline
129 & 1 & 0.003290713 & 0.003290713 & 0.003290713 & 0.003290713 \\ \hline
70 & 1 & 0.005570029 & 0.005570029 & 0.005570029 & 0.005570029 \\ \hline
35 & 2 & 0.007592412 & 0.007776318 & 0.007776318 & 0.007960224 \\ \hline
9 & 1 & 0.055467716 & 0.055467716 & 0.055467716 & 0.055467716 \\ \hline
7 & 1 & 0.028241882 & 0.028241882 & 0.028241882 & 0.028241882 \\ \hline
5 & 2 & 0.07015485 & 0.082214016 & 0.082214016 & 0.094273183 \\ \hline
4 & 2 & 0.046380154 & 0.061290142 & 0.061290142 & 0.076200129 \\ \hline
3 & 3 & 0.133671949 & 0.688292564 & 0.965164821 & 0.966040924 \\ \hline
2 & 14 & 0.127383235 & 0.369653196 & 0.336881761 & 0.949418099 \\ \hline
1 & 151 & 0.141036436 & 0.753198139 & 0.900426437 & 0.972637597 \\ \hline
\end{tabular}

\caption{Descriptive statistics for the matched data. The column \texttt{num\_matches} lists the (distinct) count of control units matched to a treated unit, from largest to smallest. The column \texttt{num\_treated} reports the number of treated units with the corresponding count of matched controls. The sum of this column is 179, i.e., the number of treated units in the trimmed sample. The sum of the row-wise product of the first two columns is 700. The remaining columns present min/average/median and max value of the pscore for the treated units with the corresponding count of matched controls. Example A: 1 treated unit is matched to 209 distinct control units, this unit's pscore is 0.000652575. Example B: 14 treated units are each matched to exactly two control units, the maximum pscore among these 14 units is 0.949418099. Example C: 151 treated units are each matched to exactly one control unit, the average of these treated units' pscores is 0.753198139.}
\label{tab:matched-set-ds}
\end{table}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  (10 p) Use \texttt{Matching::MatchBalance()} to document balance in
  the covariates (those in the Logit specification for the pscore model)
  before and after matching. Comment on your findings: a) Are covariates
  more/less balanced after matching? b) Are covariates in the matched
  sample sufficiently balanced to make you trust that the NNM estimator
  is not confounded by differences in observables?

  \textcolor{gray}{\textbf{Programming guidance:} Use 500 bootstrap replication, i.e., set \texttt{nboots=500}. Say that you name \texttt{nnm\_balance} the object (a list of lists) returned by the call to \texttt{Matching::MatchBalance()}. Apply \texttt{names()} to see the named element of the list. To check that your implementation is correct: the averages of \texttt{age} in the trimmed sample before matching and in the matched sample (i.e., after matching) are, respectively:
   \begin{itemize}
   \item \texttt{nnm\_balance\$BeforeMatching[[age\_loc]]\$mean.Tr = 25.76536}, 
   \item \texttt{nnm\_balance\$BeforeMatching[[age\_loc]]\$mean.Co = 30.96161},
   \item \texttt{nnm\_balance\$AfterMatching[[age\_loc]]\$mean.Tr = 25.76536},
   \item \texttt{nnm\_balance\$AfterMatching[[age\_loc]]\$mean.Co = 26.1724}
   \end{itemize}
  where, in our script, \texttt{age\_loc} is the number that identifies the location of the information for the covariate \texttt{age}.}
\end{enumerate}

\textbf{Solution:} We use the below script to document balance in the
covariates before and after matching, and verify that the averages of
\texttt{age} in the trimmed sample before matching and in the matched
sample (i.e., after matching) are as expected. We choose to hide the
rest of the output for convenience of the grader.

In general the covariates seem to be significantly more balanced than
before, with our T-test p-values increasingly drastically, moving from
values that were near 0 to consistently failing to reject the null
hypothesis that the covariates are balanced between the treated and
control groups. Some variables, like \texttt{black} and
\texttt{nodegree} didn't quite move to above the 0.05 thresholds,
however, they are significantly more balanced after matching. Overall,
the covariances in the matched sample seem to be sufficiently balanced
to make us trust that the NNM estimator is not confounded by differences
in most observable pre-treatment variables, with the exception of the
\texttt{hisp} covariate which still exhibits significant imbalance at
the 0.05 level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\StringTok{"treat \textasciitilde{} 1 + age + I(agesq) + edu + I(edusq) + }
\StringTok{                       married + nodegree + black + hisp + re74 +}
\StringTok{                       re75 + I(re74sq) + I(re75sq) + u74:black"}\NormalTok{)}

\NormalTok{nnm\_balance }\OtherTok{\textless{}{-}}\NormalTok{ Matching}\SpecialCharTok{::}\FunctionTok{MatchBalance}\NormalTok{(formula, }
                                      \AttributeTok{data=}\NormalTok{trimmed\_df,}
                                      \AttributeTok{match.out=}\NormalTok{matched,}
                                      \AttributeTok{nboots=}\DecValTok{500}\NormalTok{)}
\NormalTok{nnm\_balance}\SpecialCharTok{$}\NormalTok{BeforeMatching[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{mean.Tr }\CommentTok{\# 25.76536}
\NormalTok{nnm\_balance}\SpecialCharTok{$}\NormalTok{BeforeMatching[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{mean.Co }\CommentTok{\# 30.96161}
\NormalTok{nnm\_balance}\SpecialCharTok{$}\NormalTok{AfterMatching[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{mean.Tr }\CommentTok{\# 25.76536}
\NormalTok{nnm\_balance}\SpecialCharTok{$}\NormalTok{AfterMatching[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{mean.Co }\CommentTok{\# 26.1724}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  (6 p) The estimate of ATT based on the experimental data is \$1,794
  with SE \$633 (see PSet 3). Consider the estimates of ATT in Table
  \ref{tab:Tab25-4_2}: What are the takeaways? Taking inspiration from
  Imbens (2015)'s recommendations, are there actions you may want to
  take to gain more confidence in your ability to learn about the true
  causal impact of the offer of training via matching when you only have
  access to observational data?
\end{enumerate}

\textbf{Discussion:} We discuss two major takeaways and their
implications from this exercise in estimating the ATT.

Firstly, we find that in general, our point estimates of ATT and its SE
varies significantly across the different matching estimators and
algorithms being used. This is a potential cause for concern, as it
suggests that our estimates are sensitive to the choice of matching
algorithm. Working through multiple different estimators and exploring
their theoretical background as well as their empirical results leaves
us with a better understanding of the various strengths and weakness
associated, and more importantly, with the conclusion that often times,
there is not a single ``best'' matching estimator to default to.

This realization leads us to our second main takeaway: beyond
understanding the matching estimators and their various implementations
and limitations, we also need to be aware of nuances present in the
dataset we are working with, and how these nuances may affect the
performance of the matching estimators. For example, we found that the
covariates in the matched sample seem to be sufficiently balanced to
make us trust that the NNM estimator is not confounded by differences in
most observable pre-treatment variables, with the exception of the
\texttt{hisp} covariate which still exhibits significant imbalance at
the 0.05 level. Indeed, it is tests such as the stratified balance tests
that we performed in this assignment that can allow us to gain
additional insights into our data's ``shape'' and ``style'', and in turn
allow us to choose one (or multiple) matching estimator(s) most
appropriate for the context of our investigation.

In the context of Imbens (2015)'s recommendations, we would want to
consider the following actions to gain more confidence in our ability to
learn about the true causal impact of the offer of training via matching
when we only have access to observational data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We would want to consider the trial and/or use of multiple matching
  estimators to understand the sensitivity of our results to the choice
  of matching algorithm. This would allow us to understand the
  robustness of our results and the extent to which our results are
  driven by the choice of matching algorithm.
\item
  We would want to perform trimming of the sample to remove extreme
  observations that may be driving our results. This would mitigate
  p-score imbalances and thus improve the quality of matches and thus
  the performance of our matching estimators.
\item
  Imbens suggests a step-wise MLE approach for estimating pscores - we
  would want to consider this approach to ensure that our pscores are
  estimated as accurately as possible. The more accurate our pscores,
  the better our matching estimators will perform (regardless of which
  matching algorithm we might choose).
\end{enumerate}

\newpage
\begin{figure}[!htb]
\centering
  \includegraphics[width=0.9\textwidth]{figures/figures_end.png}
  \caption{Scatter plot of \texttt{re78} against the Logit-based $\widehat{p}_{i}$ estimates, separately for the treated and the control group. Each panel also includes a fitted non-parametric regression of \texttt{re78} on $\widehat{p}_{i}$ (blue line).}
  \label{fig:propensity_vs_earnings}
\end{figure}

\end{document}


\noindent \textcolor{Maroon}{\textbf{Objective}. Part 1 showed that one of the pitfalls of the (naive) stratification matching technique is that it discards the strata where either treated or control units are absent, also some (naive) strata may have a prohibitively small number of units. This suggests an alternative way to match treated and control units: take each treated unit and search for the control unit with the closest \textit{pscore} i.e., the unit's \textcolor{ForestGreen}{nearest neighbor (NN)}. Once each treated unit is matched with a control unit, the difference between the outcomes of each treated unit and the outcome of the matched control unit is computed. The \textcolor{ForestGreen}{Nearest-Neighbor Matching (NNM)} estimate of ATT is the average of these difference. While in \textcolor{ForestGreen}{Stratification Matching} there may be treated units that are discarded because no control is available in their stratum, in NNM matching all treated units have a match. However, some of these matches may be fairly poor because for some treated units the NN may have a very different \textit{pscore}; nevertheless, they contribute to the estimation of the TE. The \textcolor{ForestGreen}{Radius Matching (RM)} and \textcolor{ForestGreen}{Kernel Matching (KM)} methods offer a solution. With RM matching, each treated unit is matched only with the control units whose pscore falls within a predefined neighborhood of the pscore of the treated unit. If the dimension of the neighborhood (the radius) is very small, it is possible that some treated units are not matched because the neighborhood does not contain control units. On the other hand, the smaller the size of the neighborhood, the better the quality of the matches. With KM each treated unit is matched with a weighted average of all control units, with weights that are inversely proportional to the distance between the pscore of treated unit and control units. Clearly, these 3 methods reach different points on the frontier of the trade-off between quality and quantity of the matches, and none of them is a priori superior. Below you first look closely at the estimators produced by these methods, then you use one them to obtain estimates of the ATT of the offer of training. You continue to use the Logit-based estimates of the pscore but revert back to the original data, i.e., before trimming.}\\

\noindent \textcolor{Maroon}{\textbf{Background: Nearest Neighbor, Radius, and Kernel Matching}.
Let $C$ (respectively $T$) denote the collection of control (respectively treated) units. Let $N^{C}$ (respectively $N^{T}$) denote the number of units in $C$ (respectively $T$). Let $C\left( i\right) $ denote treated unit $i$'s \textcolor{ForestGreen}{matching set}, i.e., the set of control units matched to $i$. Let $N_{i}^{C}$ denote the number of units in $C\left( i\right) $. NNM, RM and KM\ define $i$'s matching set as, respectively: 
\begin{eqnarray}
C^{NNM}\left( i\right) &=&\left\{ j\in C:\left\vert \widehat{p}_{j}-\widehat{p}_{i}\right\vert \leq \left\vert \widehat{p}_{j^{\prime }}-\widehat{p}_{i}\right\vert \text{ }\forall j^{\prime }\in C\right\} \text{,}
\label{C_rn} \\
C^{RM}\left( i\right) &=&\left\{ j\in C:\left\vert \widehat{p}_{j}-\widehat{p}_{i}\right\vert <r\right\} \text{ (given a choice of radius }r\text{),} \label{C_rm} \\
C^{KM}\left( i\right) &=&C\text{.}  \label{C_km}
\end{eqnarray}
\noindent In words: $C^{NNM}\left( i\right)$ includes the control unit whose estimated pscore is closest to treated unit $i$'s pscore; if more than one control unit satisfy this condition (i.e., have the exact same pscore estimate), they are all included in $C^{NNM}\left( i\right)$. Given a value of $r$, $C^{RM}\left( i\right)$ includes all the control units whose estimated pscore is within $\pm r$ of treated unit $i$'s pscore, thus $C^{RM}\left( i\right)$ may contain no control unit, one control unit, or multiple control units. Finally, $C^{KM}\left( i\right)$ includes all control units irrespective of their pscore estimate.
\noindent The NNM, RM, and KM estimators of the ATT have a common form: 
\begin{equation}
\widehat{ATT}^{m} = \frac{1}{N^{T}} \sum_{i \in T}\left[y_{i}-\sum_{j \in C^{m}\left( i\right) }w_{ij}^{m}y_{j}\right] \text{, }m \in \left\{ NNM,RM,KM\right\} \text{,}  \label{ATT_m}
\end{equation}
\noindent where $C^{m}\left( i\right) $ is defined in (\ref{C_rn})-(\ref{C_km}) and $w_{ij}^{m}$ is a matching method-specific weight:
\begin{equation}
w_{ij}^{m}=\left\{ 
\begin{array}{ll}
\frac{1}{N_{i}^{C}}\text{ if }j\in C^{m}\left( i\right) \text{ and zero
otherwise} & \text{for }m\in \left\{ NNM,RM\right\} \\ 
K\left( \frac{\widehat{p}_{j}-\widehat{p}_{i}}{h}\right)
/\sum_{k \in C}K\left( \frac{\widehat{p}_{k}-\widehat{p}_{i}}{h}\right) & \text{for }m=KM\text{ (given kernel function }K\left(\cdot \right) \text{ and bandwidth }h \text{)}
\end{array}
\right. \text{.}  \label{w_ij}
\end{equation}
\noindent In words: Pick a treated unit $i$. Taken together expressions (\ref{ATT_m}) and (\ref{w_ij}) say that, in both NNM and RM, unit $i$'s (counterfactual) outcome without treatment is proxied by the sample average of the observed outcome of the controls included in $i$'s matching set: you have this result because, per expression (\ref{w_ij}), all control units in $i$'s matching set have weight equal to 1 divided by the number of control units in $i$'s matching set. For example, suppose that when you implement NNM the matching set of each treated unit includes exactly one NN. In this special case, all matched control units have weight equal to $1=\frac{1}{1}$, which implies that NNM proxies unit $i$'s (counterfactual) outcome without treatment by the observed outcome experienced by $i$'s unique NN. \\ \\
To understand the expression of the KM weights we next review the concept of \textcolor{ForestGreen}{kernel function}. A kernel function is a non-negative valued function $K(\cdot)$ with the following properties: 1) $K\left( z \right) = K\left( -z \right)$, i.e $K\left(\cdot \right) $ is symmetric around zero; 2) $\int K\left( z \right) dz =1$, i.e., it integrates to 1; 3) $\int z ^{2}K\left(z \right) dz \not=0$ and finite; and 4)$\int K^{2}\left( z \right) dz <\infty $. An example of a kernel function is the standard normal (aka Gaussian) pdf: $K\left( z\right) =\phi(z)=\frac{1}{\sqrt{2\pi }}\exp \left( -\frac{z^{2}}{2}\right)$. It is immediate to verify that it satisfies all the properties because $\phi(\cdot)$ 1) is symmetric around its zero mean, 2) integrates to 1, 3) has variance equal to 1; and 4) the integral of $\phi(\cdot)^2$ is $\frac{1}{2\sqrt{\pi}}$. Recall also that $\phi(\cdot)$ is approximately zero when evaluated at points smaller than -3 or larger than +3, and its mode (i.e., maximum value) is attained at point 0: $\phi(0)=\frac{1}{\sqrt{2\pi}}$. With this information in hands we return to expression (\ref{w_ij}). To start simple, let $h=1$, so that the weight expression reduces to $w_{ij}=\phi \left(\widehat{p}_{j}-\widehat{p}_{i}\right)/\sum_{k \in C}\phi \left( \widehat{p}_{k}-\widehat{p}_{i}\right)$. The denominator is the same for all $j \in C$ (it only varies across treated units). Consider two control units $j$ and $j'$. Which of the two control units is given more weight when proxying unit $i$'s outcome without treatment? The answer depends on the numerator of the weight expression, i.e.,  $\phi \left(\widehat{p}_{j}-\widehat{p}_{i}\right)$ and $\phi \left(\widehat{p}_{j'}-\widehat{p}_{i}\right)$ respectively. In particular, if $\widehat{p}_{j}=\widehat{p}_{i}$, unit $j$ is given maximal weight (per above equal to $\frac{1}{\sqrt{2\pi}}$), and if $\widehat{p}_{j'}-\widehat{p}_{i}$ is smaller than -3 or larger than +3, unit $j'$ is given approximately zero weight. This shows that in Gaussian KM all control units are given a non zero weight: the nearest ones (in terms of pscore) are given the larger weight, those units whose pscore is quite far from the treated unit $i$'s pscore value are given an infinitesimally small weight. We conclude by commenting on $h$: this parameter is called \textcolor{ForestGreen}{bandwidth} and is chosen by the researcher. Intuitively, a very small value of $h$ implies that only the control units whose pscore value is extremely close to that of treated unit $i$ are given non negligible weight; conversely, a very large value of $h$ implies that even control units whose pscore value is extremely far from that of treated unit $i$ are given non negligible weight. For the KM approach to perform well the researcher wants $h$ to become smaller as the sample size increases as this guarantees that the control units given non negligible weights are only those whose pscore is extremely close to the treated unit's pscore.
}

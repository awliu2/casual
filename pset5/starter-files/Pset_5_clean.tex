%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ECMA 31360 PSet 5: Causal Inference with Observational Data
% Author: Melissa Tartari
% Created: 10/13/2023
% Last Updated: 01/25/2024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage[]{graphicx}
\usepackage{optidef}
\usepackage{listings}
\usepackage{enumitem}

\setcounter{MaxMatrixCols}{10}

% https://tex.stackexchange.com/questions/10497/how-do-i-robustly-typeset-a-double-hat
\usepackage{accents}
\newlength{\dhatheight}
\newcommand{\doublehat}[1]{%
    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
    \addtolength{\dhatheight}{-0.35ex}%
    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
    \smash{\hat{#1}}}}

\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\usepackage{alltt}

\textwidth=7.6in
\textheight=9.9in
\topmargin=-1.2in
\headheight=0in
\headsep=.5in
\hoffset  -1.25in

\newcommand{\psetyear}{2024}
\newcommand{\psetnum}{5}

\usepackage{fancyhdr}
\pagestyle{fancy} 
\fancyhf{} % clear all header and footer fields
\fancyfoot[R]{\color{Gray}{ECMA 31360 PSet \psetnum, Page \thepage}}

\usepackage{lineno}
\linenumbers


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ECMA 31360, Pset \psetnum: Estimation of TEs Using Matching Methods}
\date{}
\author{Melissa Tartari}
\maketitle
\thispagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Premise
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textcolor{Maroon}{\textbf{Objective}: In PSet 3 you used NSW experimental data to estimate the treatment effect of the offer of training on post-training earnings. In PSet 4 you used datasets created to mimic observational data and reported estimates of the treatment effect obtained using regression-based methodologies. In this pset you complete STEP 1 in \textcolor{ForestGreen}{propensity score matching (PSM)}. Specifically, you estimate each sample unit's probability of being included in the treatment group, which we call the \textcolor{ForestGreen}{propensity score}, or \textcolor{ForestGreen}{pscore} for short. In the next PSet you use the estimated pscores to estimate the impact of the offer of training on post-intervention earnings using observational data.} \\

\noindent \textcolor{Maroon}{\textbf{Background}: A unit's pscore is \underline{known} when the data results from an experiment because it is set by the researcher and used to assign units to one group or the other. For example, in an RCT with random assignment (RA) the researcher flips (say) a balanced coin to decide whether to assign a unit to treated or control groups. That is, the researcher uses an assignment probability equal to 0.5. Thus, each unit's pscore is 0.5. As another example consider an RCT with conditional random assignment (CRA) such that units are either males or females and the researcher assigns males to the treated group with probability 0.5 but they assign females to the treated group with probability 0.3. Then the pscore of any male sample unit is 0.5 and the pscore of any female sample unit is 0.3.}\\

\noindent \textcolor{Maroon}{When the data is observational, a unit's pscore is \underline{not known} hence you need to first estimate it in order to use it in matching procedures. In this PSet you estimate the pscore based on two models: the \textcolor{ForestGreen}{Linear Probability Model (LPM)} and the \textcolor{ForestGreen}{Logit Model}. The LPM posits that, as a function of the unit's observable and predetermined characteristics $\mathbf{x}_i=(1, x_{i,1},\ldots,x_{i,K})$, the probability of ``success'' (with ``success'' = being assigned to the treatment group) is a linear-in-parameter function\footnote{This explains the name ``LPM'': it \textit{models} the \textit{probability} of “success” as a \textit{linear}-in-parameter function. The ``linearity'' in the name of this model pertains to the way in which parameters enter the specification. The OPVs may enter linearly or non-linearly. For example, say that there are two covariates $(x_{1i},x_{2i})$, then the model $\Pr(D_i=|\mathbf{x}_i=\mathbf{x})=\theta_0+\theta_1 x_{1}+\theta_2 x_{2}+\theta_3 x_{1}^2+\theta_4 ln(x_{1})$ is a LPM. This means that, when in the text we write, e.g., $\Pr(D_i=|\mathbf{x}_i=\mathbf{x})=\theta'\mathbf{x}$, the RHS variables may be transformation of the original OPVs.}:
\begin{equation}\label{eq:lpm}
\Pr(D_i=1|\mathbf{x}_i=\mathbf{x})=\theta'\mathbf{x}, \text{ for some } \theta=(\theta_0,\theta_1, \ldots, \theta_K).
\end{equation}
\noindent Instead, the Logit model posits that, as a function of the unit's characteristics, the probability of ``success'' is a non-linear function of the parameters of the form:
\begin{equation}\label{eq:logit}
\Pr(D_i=1|\mathbf{x}_i=\mathbf{x})=\frac{e^{\gamma'\mathbf{x}}}{1 + e^{\gamma'\mathbf{x}}}, \text{ for some } \gamma=(\gamma_0,\gamma_1, \ldots, \gamma_K).
\end{equation}
\noindent You know the LPM from introductory econometrics. Why? You verified in PSet 1 that: 
\begin{equation}\label{eq:meanDeqPr}
E[D_i|\mathbf{x}_i=\mathbf{x}]=\Pr(D_i=1|\mathbf{x}_i=\mathbf{x}).
\end{equation}
Taken together expressions (\ref{eq:lpm}) and (\ref{eq:meanDeqPr}) say that the mean of $D_i$ is a \underline{linear-in-parameter} function, i.e., 
\begin{equation}\label{eq:meanLPM}
E[D_i|\mathbf{x}_i=\mathbf{x}]=\theta'\mathbf{x}. 
\end{equation}
This means that the LPM is a standard multivariate linear regression model (MLRM) whose dependent variable is the binary 0/1 variable $D_i$. Thus, you can use OLS to estimate the parameters of a LPM, in your case $\theta$. In this Pset you do that and you also use a second estimation methodology suitable for linear-in-parameter models, \textcolor{ForestGreen}{Lasso regression}. (No prior knowledge of Lasso is expected, we guide you through implementation.)}\\

\noindent \noindent \textcolor{Maroon}{Next, using expressions (\ref{eq:logit}) and (\ref{eq:meanDeqPr}) you have:
\begin{equation}\label{eq:meanLogit}
E[D_i|\mathbf{x}_i=\mathbf{x}]=\frac{e^{\gamma'\mathbf{x}}}{1 + e^{\gamma'\mathbf{x}}}.
\end{equation}
This says that according to the Logit model the mean of $D_i$ is a \underline{non-linear} function of the parameters. Thus, you cannot apply OLS nor Lasso to estimate $\gamma$. Instead you use a methodology called \textcolor{ForestGreen}{Maximum Likelihood Estimation (MLE)}. (No prior knowledge of MLE is expected, we guide you through implementation.) By the end of this PSet you will have estimated the pscore for all the units included in the \texttt{nswpsid.csv} dataset.}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Part 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
{\LARGE Part 1: Estimate the PScore Using A Linear-in-Parameter Model (65 p)}
\end{center}

\noindent \textcolor{Maroon}{\textbf{Background: Lasso}. Our objective is to estimate $\theta$ in the linear-in-parameter model for the conditional expectation function given in expression (\ref{eq:meanLPM}), i.e., $E[D_i|\mathbf{x}_i=\mathbf{x}]=\theta'\mathbf{x}$. The \textcolor{ForestGreen}{Lasso} regression is a type of linear regression that uses \textcolor{ForestGreen}{shrinkage} (also called \textcolor{ForestGreen}{regularization}) to estimate $\theta$.\footnote{Lasso is used in a journal article by Guido Imbens (2021 Nobel Prize) which we cover in CAUS\_3\_Observational.pdf} The Lasso procedure encourages simple models with few parameters by forcing some of the coefficient estimates to be exactly equal to zero. To appreciate the difference between OLS and Lasso it is useful to compare their objective functions:
\begin{argmini}
{\theta}{\sum_{i=1}^{n}(D_i-\mathbf{x}_{i}'\theta)^2}{\label{eq:ols:objfnc}}{\text{OLS:  } \hat{\theta}=}
\end{argmini}
\begin{argmini}
{\theta}{\sum_{i=1}^{n}(D_i-\mathbf{x}_{i}'\theta)^2 + \lambda\sum_{k=1}^{K}|\theta_k|}{\label{eq:lasso:objfnc}}{\text{Lasso:  } \doublehat{\theta}=}
\end{argmini}
\noindent You are familiar with the minimization problem in expression (\ref{eq:ols:objfnc}): OLS minimizes the total sum of squared deviations. In expression (\ref{eq:lasso:objfnc}), $|\theta_k|$ is the absolute value of $\theta_k$ and $\lambda \geq 0$ is a pre-specified parameter that determines the degree of ``regularization''; it is called a \textcolor{ForestGreen}{``penalization" parameter} and is ``tuned'' (i.e., selected) by the researcher via \textcolor{ForestGreen}{cross-validation (CV)}.\footnote{ECMA 31350 covers CV in detail. Here is some basic intuition. When using CV to find the best $\lambda$ in Lasso regression (or other regularized regression methods), the typical objective is to minimize the prediction error on the validation data. Here's a step-by-step breakdown:
\begin{enumerate}
%%%%
\item Split the Data: Divide your dataset into two halves (folds). Let's call them Fold 1 and Fold 2.\label{item:lambda-split}
\item Iterate Over Lambda Values: For a range of lambda values:\label{item:lambda-iter}
	\begin{enumerate}
	\item Using Fold 1 for Validation:\label{item:lambda-iter-fold1}
		\begin{enumerate}
			\item Training: Fit a Lasso model using Fold 2 as your training data with the current lambda value.
			\item Validation: Predict the outcomes for Fold 1 using the model trained on Fold 2.
			\item Compute the Error: Calculate the prediction error for Fold 1. This could be Mean Squared Error (MSE) for regression problems or other relevant metrics for different types of problems. Recall that the MSE is the average of the squared difference between the value of the dependent variable in the data and its predicted (fitted) value.
		\end{enumerate}
        \item Using Fold 2 for Validation:\label{item:lambda-iter-fold2}
		\begin{enumerate}
			\item Training: Fit a Lasso model using Fold 1 as your training data with the current lambda value.
			\item Validation: Predict the outcomes for Fold 2 using the model trained on Fold 1.
			\item Compute the Error: Calculate the prediction error for Fold 2. 
		\end{enumerate}
	\item Average Error for Lambda: Take the average of the prediction error from steps \ref{item:lambda-iter-fold1} and \ref{item:lambda-iter-fold2} for the current lambda value.
	\end{enumerate}
	\item Select Lambda with Minimum Error: After iterating over all lambda values, select the lambda that gives the lowest average prediction error across the two folds.
\end{enumerate}
The intuition behind this approach is to find a $\lambda$ value that, on average, provides the best predictive performance on data it hasn't seen during training. By minimizing the prediction error on the validation data, you're aiming to select a model that generalizes well to new data.
} 
Mentally verify that: (a) if $\lambda=0$, the two minimization problems coincide, hence $\hat{\theta}=\doublehat{\theta}$; instead (b) a very large $\lambda$ drives most of the Lasso's slope estimates to zero. This explains why Lasso penalizes a linear regression model for having ``too many'' covariates.\footnote{The acronym ``LASSO'' stands for Least Absolute Shrinkage and Selection Operator. ``Shrinkage'' describes the fact that some coefficients' estimates are made smaller compared to OLS, and ``selection'' describes the fact that some coefficient estimates are set to zero.} Lasso is often used when $K$ is large (and might even be larger than $n$). Below you obtain $\hat{\theta}$ and $\doublehat{\theta}$.}

\begin{enumerate}[label=\textbf{Q\arabic{enumi}}.,ref=Q\arabic{enumi}, wide=0pt, itemsep=0em, topsep=5pt, labelindent=0pt]
%%%%%%%%%%%%%%%%%
\item (5 p) To familiarize yourself with the LPM here you practice with this model in a simplified setting. Suppose that there is only one OPV $x_i$, and $x_i$ takes values $\{1,\ldots,M\}$ (e.g., your data only includes education, and this OPV ranges from 1 year to 12 years). Suppose that you use the following LPM specification for the conditional expectation of treatment status as a function of education: $E[D_i|x_i]=\sum_{m=1}^{M}\theta_m1[x_i=m]$. Find $\hat{p}_i$ for each sample unit. \textcolor{gray}{\textbf{Hint}: this is a paper and pencil exercise, you do not have to write any R script. You want to suggest an estimator for $\theta=(\theta_1,\ldots,\theta_m)$, get the estimates, then use the estimates to get the implied fitted value $\hat{p}_i$ for each $i$.}

%%%%%%%%%%%%%%%%%
\item (45 p) Consider the LPM introduced in expression (\ref{eq:lpm}) and recall the (implied) expression (\ref{eq:meanLPM}) for the conditional mean of treatment status. Here you estimate $\theta$ given a specific form of $\theta'\mathbf{x}_{i}$ and using the OLS approach.\label{item:lpm:ols}
\begin{enumerate}
%%%%%%
\item (10 p) Use the \texttt{nswpsid.csv} data with: 1) \texttt{treat} as $D_i$; 2) a constant and the following 13 covariates as $\mathbf{x}_{i}$: \texttt{age, agesq, edu, edusq, married, nodegree, black, hisp, re74, re75, re74sq, re75sq, u74black}, where e.g. \texttt{re74sq} is the square of \texttt{re74} and \texttt{u74black} is the interaction b/w \texttt{u74} and \texttt{black}. Obtain the OLS estimator of $\theta=(\theta_0,\theta_1,\ldots,\theta_{13})$, denoted $\hat{\theta}$. \textcolor{gray}{\textbf{Programming Guidance:} Declare a formula object with \texttt{as.formula( )} then use \texttt{stats::lm( )}. For later reference, in our R script we named the formula object \texttt{pscore\_formula}.} \label{item:lpm:ols:reg}
%%%%%%
\item (23 p) Use $\hat{\theta}$ from \textbf{\ref{item:lpm:ols:reg}} to describe the change in the pscore associated with a change in one of the covariates. Specifically:
\begin{enumerate}
\item (2 p) Show that $\frac{\partial Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial x_{i,k}} = \frac{\partial E[D_i|\mathbf{x}_{i}=\mathbf{x}]}{\partial x_{i,k}}= \frac{\partial \mathbf{x}'\theta}{\partial x_{i,k}} = \theta_k$ for any $k >0$ and $x_{i,k}$ that varies continuously and such that no other covariates mechanically changes when $x_{i,k}$ changes. \textcolor{gray}{\textbf{Hint:} Use Calculus.}
\item (4 p) Write out the expression for $\frac{\partial Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial \texttt{re75}}$ paying attention to the fact that earnings in 1975 contribute to two mechanically related regression covariates: \texttt{re75} and \texttt{re75sq}. \textcolor{gray}{\textbf{Hint:} Use Calculus.}\label{item:lpm:delta-pscore-re75}
\item (17 p) Use $\hat{\theta}$ and the expression from \textbf{\ref{item:lpm:delta-pscore-re75}} to complete this sentence: ``\textit{Evaluated at average 1975 earnings, a \$10,000 increase in 1975 earnings is associated with [\dots] percentage points [lower/higher] probability of being included in the treated group}.'' \textcolor{gray}{\textbf{Hint:} Start by computing average 1975 earnings.}
\end{enumerate}
%%%%%%
\item (6 p) As mentioned above, when $D_i$ indicates whether a unit belongs to the treated or the control group we call it the \textcolor{ForestGreen}{treatment status indicator} and we call the probability of ``success'', namely $Pr(D_i=1|\mathbf{x}_i=\mathbf{x})$, the \textcolor{ForestGreen}{propensity score}. Use the regression output from \textbf{\ref{item:lpm:ols:reg}} to obtain \textcolor{ForestGreen}{fitted values}\footnote{Consider a generic MLRM regression framework where $y_i$ denotes the dependent variable, $\mathbf{x}_i$ are the covariates (including a constant), and $\beta$ are the coefficients. Let $\hat{\beta}$ denote the OLS estimates. We define the fitted value of the dependent variable as $\hat{y}_i \equiv \hat{\beta}'\mathbf{x}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i}+\ldots+\hat{\beta}_K x_{i,K}$.}, denoted $\hat{p}_i$. Note: Interpret $\hat{p}_i$ as the estimated probability that a unit $i$ with pre-treatment characteristics $\mathbf{x}_i$ is treated. \textcolor{gray}{\textbf{Programming Guidance:} Apply \texttt{stats::predict( )} to the object returned by \texttt{stats::lm( )} to get fitted values.}\label{item:lpm:ols:fitted}
\item (2 p) Add $\hat{p}_i$ obtained in \textbf{\ref{item:lpm:ols:fitted}} to your dataframe as a column. \textcolor{gray}{\textbf{Programming Guidance:} Use \texttt{dplyr::mutate( )}}.
\item (4 p) The LPM has one disadvantage: $\hat{p}_i$ is not guaranteed to be between 0 and 1 (the range of values a probability may take). For how many units is $\hat{p}_i$ obtained in \textbf{\ref{item:lpm:ols:fitted}} negative or larger than 1?
\end{enumerate}

%%%%%%%%%%%%%%%%% Lasso
\item (15 p) In \textbf{\ref{item:lpm:ols}} you estimated $\theta$ using OLS. Here you use \textcolor{ForestGreen}{Lasso} regression. Specifically,
\begin{enumerate}
\item (10 p) Obtain $\doublehat{\theta}$. \textcolor{gray}{\textbf{Programming Guidance:} In our R script the formula is called \texttt{pscore\_formula} and the dataframe \texttt{dt\_psid}. To implement the Lasso approach you need to take a few steps: 1) prepare two objects \texttt{x} and \texttt{y} storing, respectively, the covariates and the dependent variable of the regression; 2) find the value of $\lambda$ to plug in the minimization problem (\ref{eq:lasso:objfnc}); 3) use \texttt{glmnet::glmnet(..., alpha = 1)} to fit the Lasso model (glm stands for \textcolor{ForestGreen}{generalized linear model}). Specifically, you want to incorporate in your R script the following lines of code:}
\begin{lstlisting}[language=R]
# Estimate the pscore using Lasso
# Predictor variables
x <- stats::model.matrix(pscore_formula, data = dt_psid)[,-1] 
# Outcome variable
y <- dt_psid$treat 
# Find the best lambda using cross-validation
set.seed(123) 
cv_lasso <- glmnet::cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
cv_lasso$lambda.min
# Apply Lasso
lasso_propensity <- glmnet::glmnet(x = x, y = y, alpha = 1,  standardize = TRUE,
                                   lambda = cv_lasso$lambda.min)
\end{lstlisting}
\item (5 p) Compare $\hat{\theta}$ and $\doublehat{\theta}$. Are there coefficient estimates that Lasso sets to zero but OLS does not? \textcolor{gray}{\textbf{Programming Guidance:} View the coefficient estimates from \texttt{lm( )} and \texttt{glmnet:glmnet( )} by applying method \texttt{coef( )}, e.g. \texttt{coef(lasso\_propensity)} where in our script \texttt{lasso\_propensity} is the object returned by \texttt{glmnet::glmnet( )}}.
\end{enumerate}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Part 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
{\LARGE Part 2: Estimate the Propensity Score Using the Logit Model (35 p)}
\end{center}



\noindent \textcolor{Maroon}{\textbf{Background: Maximum Likelihood Estimation}: MLE returns the values of the parameters that maximize the joint distribution of the data, that is, that make the data as ``most likely'' of having been generated by a parametric model.\footnote{You should have covered MLE estimation of LRMs in introductory econometrics, and MLE more generally in any introductory statistics class. In case you didn't, we offer a brief introduction as applied to the setting under consideration.} It is useful to clearly differentiate between the RVs in a RS and their realized values: denote $\tilde{D_i}$ the realized value of $D_i$ and similarly $\tilde{\mathbf{x}}_i$ the realized value of $\mathbf{x}_i$. The joint distribution of the data is therefore: $\Pr(D=\tilde{D}|\mathbf{X}=\tilde{\mathbf{X}};\gamma)$ where $D$ collects all the $D_i$'s, $\tilde{D}$ collects all the $\tilde{D_i}$'s, $\mathbf{X}$ collects all the $\mathbf{x}_i$'s, and $\tilde{\mathbf{X}}$ collects all the $\tilde{\mathbf{x}}_i$'s. $\Pr(D=\tilde{D}|\mathbf{X}=\tilde{\mathbf{X}};\gamma)$ is called the \textcolor{ForestGreen}{Likelihood function} and is often denoted $\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})$ to underscore that we want to think of it as a function of the parameters $\gamma$ for given realization of the data. When we work with a RS we have independence, which means that the joint distribution factors into the product of $N$ terms: 
\begin{equation}
\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})= \prod_{i=1}^{N}\Pr(D_i=\tilde{D}_i|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)
\end{equation}
Because $\tilde{D}_i$ takes value either 0 or 1, we can also write:
\begin{equation}
\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})= \prod_{i=1}^N\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)^{\tilde{D}_i}\times \Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)^{1-\tilde{D}_i}.
\end{equation}
It is common to work with the logarithm of this function, called the \textcolor{ForestGreen}{Log-Likelihood function}:
\begin{equation}
\begin{aligned}
\ell(\gamma; \tilde{D}, \tilde{\mathbf{X}}) &\equiv ln(\mathcal{L}(\gamma; \tilde{D}, \tilde{\mathbf{X}})) \\
&=\sum_{i=1}^{N}\Big[\tilde{D}_i \times ln(\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma))+(1-\tilde{D}_i)\times ln(\Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma)) \Big], \\
&=\sum_{i \text{ s.t.} \tilde{D}_i=1}ln(\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma))+\sum_{i \text{ s.t.} \tilde{D}_i=0}ln(\Pr(D_i=0|\mathbf{x}_i=\tilde{\mathbf{x}}_i;\gamma)).
\end{aligned}
\end{equation}
Then the MLE estimator of $\gamma$ is: 
\begin{argmaxi}
{\gamma}{\ell(\gamma; \tilde{D}, \tilde{\mathbf{X}}).}{\label{eq:mle:objfnc}}{\hat{\gamma}=}
\end{argmaxi}
In the special case of the Logit model $\Pr(D_i=1|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)=\frac{e^{\tilde{\mathbf{x}}_i'\gamma}}{1+e^{\tilde{\mathbf{x}}_i'\gamma}}$ and $\Pr(D_i=0_i|\mathbf{x}_i=\tilde{\mathbf{x}}_i; \gamma)=\frac{1}{1+e^{\tilde{\mathbf{x}}_i'\gamma}}$. Hence the MLE estimator of $\gamma$ in the Logit model under consideration is:
\begin{argmaxi}
{\gamma}{\sum_{i \text{ s.t. }\tilde{D}_i=1} ln\big[\frac{e^{\tilde{\mathbf{x}}_i'\gamma}}{1+e^{\tilde{\mathbf{x}}_i'\gamma}} \big] + \sum_{i \text{ s.t. }\tilde{D}_i=0} ln\big[\frac{1}{1+e^{\tilde{\mathbf{x}}_i'\gamma}} \big].}{\label{eq:logit:mle:objfnc}}{\hat{\gamma}=}
\end{argmaxi}
Thus, to find $\hat{\gamma}$ you solve the system of first order conditions; this system has as many unknowns as there are elements in $\gamma$.}


\begin{enumerate}[label=\textbf{Q\arabic{enumi}}.,ref=Q\arabic{enumi}, wide=0pt, itemsep=0em, topsep=5pt, labelindent=0pt, resume]
%%%%%%%%%%%
\item (5 p) To familiarize yourself with MLE, here you practice with this estimation approach in a simplified setting. Suppose that there are no OPVs and that your sample has size $N=100$, and 10 units have $\tilde{D}_i=1$ and 90 units have $\tilde{D}_i=0$. Assume that each $D_i$ has a Bernoulli distribution with parameter $\gamma$. Write the likelihood function, take FOCs, and solve the FOC system (which has just one equation in one unknown) by paper and pencil. What you obtain is the MLE estimate of $\gamma$. \textcolor{gray}{\textbf{Hint}: You know, intuitively, that the answer must be $\hat{\gamma}=\frac{1}{10}$.}
%%%%%%%%%%%
\item (20 p) Consider the Logit model introduced in expression (\ref{eq:logit}). We mentioned that the Logit model yields a conditional mean of $D_i$ that is not linear in parameters. Here you estimate $\gamma$ using the MLE approach.\begin{enumerate}
\item (10 p) Continue using the \texttt{nswpsid.csv} dataset, and the variable definitions provided in \textbf{\ref{item:lpm:ols:reg}}. Obtain the MLE estimator of $\gamma= (\gamma_0,\gamma_1,\dots,\gamma_K)$, denoted $\hat{\gamma}$. \textcolor{gray}{\textbf{Programming Guidance:} Use \\ \texttt{stats::glm(pscore\_formula, family = binomial( ), data = psid\_tb)} where \texttt{family = binomial( )} tells R that you want to estimate a Logit model; \texttt{psid\_tb} is the name of the dataframe in our R script.}\label{item:logit:reg}

%%%%%%%%%%%
\item (10 p) Use the estimated coefficients to describe the change in the pscore associated with a change in one of the covariates. Specifically:
\begin{enumerate}
\item (3 p) Show that $\frac{\partial \Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial x_{i,k}}=\frac{\partial \frac{e^{\mathbf{x}'\gamma}}{1+ e^{\mathbf{x}'\gamma}}}{\partial x_{i,k}}= \gamma_k \frac{e^{\mathbf{x}'\gamma}}{(1+e^{\mathbf{x}'\gamma})^2}$ for any $k >0$ and $x_{i,k}$ that varies continuously and such that no other covariates mechanically changes when $x_{i,k}$ changes. \textcolor{gray}{\textbf{Hint:} Use Calculus.}\label{item:logit:partial}
\item (7 p) Write out the expression for $\frac{\partial \Pr(D_i=1|\mathbf{x}_{i}=\mathbf{x})}{\partial \texttt{re75}}$ paying attention to the fact that earnings in 1975 contribute to two mechanically related regression covariates: \texttt{re75} and \texttt{re75sq}. \textcolor{gray}{\textbf{Hint:} Use Calculus.}\label{item:logit:delta-pscore-re75}
\end{enumerate} 
\end{enumerate} 
%%%%%%
\item (4 p) Use the regression output from \textbf{\ref{item:logit:reg}} to obtain \textcolor{ForestGreen}{fitted values} denoted $\hat{p}_i^{Logit}$. Note that you interpret $\hat{p}_i^{Logit}$ as the estimated probability that a unit $i$ with pre-treatment characteristics $\mathbf{x}_i$ is treated. \textcolor{gray}{\textbf{Programming Guidance:} Apply \texttt{stats::predict( )} with argument \texttt{type = ``response"} to the object returned by \texttt{stats::glm( )}}.\label{item:logit:fitted}
\item (2 p) Add $\hat{p}_i^{Logit}$ to your dataframe as a column (you will use it in PSet 6 to estimate average TEs by PSM).
\item (4 p) The Logit has one advantage over the LPM: $\hat{p}_i^{Logit}$ is guaranteed to be strictly between 0 and 1. Why is that?
\end{enumerate}

\end{document}

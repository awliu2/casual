% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{amsmath}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Problem Set 4},
  pdfauthor={Tessie Dong, Derek Li, Andi Liu},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Problem Set 4}
\author{Tessie Dong, Derek Li, Andi Liu}
\date{Due Jan 26th, 2024}

\begin{document}
\maketitle

\begin{center}
{\LARGE Part 1: Describe the Data (10 p)}
\end{center}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fill Table \ref{tab:descriptive-stats}'s columns 5 and 6 using,
  respectively, the data in \texttt{nswpsid.csv} and in
  \texttt{nswcps.csv}.
  \textcolor{Gray}{\textbf{Notes}: You want to limit attention to observations with \texttt{treat=0}. You filled columns 3 and 4 in PSet 3.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load data}
\NormalTok{nswpsid }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"starter{-}files/nswpsid.csv"}\NormalTok{)}
\NormalTok{nswcps }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"starter{-}files/nswcps.csv"}\NormalTok{)}
\NormalTok{nswpsid\_treat0 }\OtherTok{\textless{}{-}}\NormalTok{ nswpsid }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\NormalTok{nswcps\_treat0 }\OtherTok{\textless{}{-}}\NormalTok{ nswcps }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(treat }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_cps }\OtherTok{\textless{}{-}} \FunctionTok{summarise\_all}\NormalTok{(nswcps\_treat0, }\FunctionTok{list}\NormalTok{(mean))}
\NormalTok{summary\_psid }\OtherTok{\textless{}{-}} \FunctionTok{summarise\_all}\NormalTok{(nswpsid\_treat0, }\FunctionTok{list}\NormalTok{(mean))}
\end{Highlighting}
\end{Shaded}

  \input{figures/table_descriptive_stats.tex}
\item
  Briefly comment on the completed Table \ref{tab:descriptive-stats}.
  \textcolor{gray}{\textbf{Hint}: Are the PSID-1 and CPS-1 samples "good" control groups?}

  \textbf{Answer:} I would argue that these samples are not the best
  control groups - this is mostly because many of the OPV covariates
  from the PSID and CPS exhibit large differences from the
  characteristics of the NSW sample. For example, the average age of the
  NSW sample is 25.82, while the average age of the PSID sample is 34.5,
  and there are large differences in income across the three samples.
  This suggests that the populations from which PSID and CPS were drawn
  are not very similar to the population of the NSW sample - making
  comparisons between treated individuals in the NSW sample and
  ``untreated'' individuals in the PSID and CPS samples less reliable,
  in our opinion.
\item
  Why do you think that Dehajia and Wahba constructed their
  ``observational datasets'' by pulling together the treated sample from
  NSW and a sample of individuals drawn from either the PSID or the CPS
  data?
  \textcolor{gray}{\textbf{Hint:} Both PSID and CPS include information on whether an individual enrolled in a training course during the previous 12 months. Thus, Dehajia and Wahba could have exploited exclusively observational variation in whether an individual enrolled in a training program. Why do you think that they chose not to follow this approach?}

  \textbf{Answer:} We believe that Dehajia and Wahba chose to pool the
  NSW and PSID/CPS datasets because they wanted to have a larger sample
  size to work with. This is because the NSW sample is relatively small,
  and the PSID/CPS samples are much larger. By pooling the NSW and
  PSID/CPS samples, Dehajia and Wahba are able to increase the sample
  size of their dataset. In addition, by analyzing samples drawn from
  different distributions (i.e.~PSID/CPS datasets), they could increase
  the generalizability of their results to the population.
\end{enumerate}

\include{figures/table_reg_specs.tex}

\pagebreak

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  (30 p) These questions pertain to the specification in expression
  (\ref{TCcomp}) thus you obtain the
  \textcolor{ForestGreen}{Treatment-Control Comparison (TCC) Estimator}
  of the treatment effect of the offer of training.\label{item:TCcomp}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    (8 p) Estimate \(\rho\).
  \item
    (10 p) Compute \textcolor{ForestGreen}{heteroschedasticity-robust}
    SEs.
  \item
    (2 p) Verify that \(\hat{\rho}\) in \textbf{\ref{item:TCcomp-rho}}
    equals \((\overline{re78}^{D=1}-\overline{re78}^{D=0})\), i.e., the
    difference between the average post-training earnings of the treated
    and of the control individuals. This fact explains the name of the
    estimator, and is consistent with what you derived in previous
    Psets.\label{item:TCcomp-diff}
  \item
    (10 p) Intuitively explain why the TCC approach may not deliver a
    credible estimate of the average effect of the treatment of
    interest.
    \textcolor{gray}{\textbf{Hint}: Use the result in  \textbf{\ref{item:TCcomp-diff}} to think about what this approach uses to proxy for the missing data, i.e., for the control units' mean of the potential outcome w/ treatment, and for the treated units' mean of the potential outcome w/out treatment.}\label{item:TCcomp-rho}
  \end{enumerate}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  See table below for estimate of \(\rho\)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Run the regression}
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(re78 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treat, }\AttributeTok{data =}\NormalTok{ nswpsid)}

\CommentTok{\# Extract coefficients}
\NormalTok{coefficients }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{coefficients[}\StringTok{"treat"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"Estimate"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{)]}

\CommentTok{\# Get additional test statistics}
\NormalTok{test\_stats }\OtherTok{\textless{}{-}}\NormalTok{ lmtest}\SpecialCharTok{::}\FunctionTok{coeftest}\NormalTok{(m1, }\AttributeTok{vcov. =} \FunctionTok{vcov}\NormalTok{(m1))}

\CommentTok{\# Combine into a data frame}
\NormalTok{result\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Estimate =}\NormalTok{ coefficients[}\StringTok{"Estimate"}\NormalTok{],}
                        \AttributeTok{Std\_Error =}\NormalTok{ coefficients[}\StringTok{"Std. Error"}\NormalTok{],}
                        \AttributeTok{t\_value =}\NormalTok{ test\_stats[}\DecValTok{2}\NormalTok{, }\StringTok{"t value"}\NormalTok{],}
                        \AttributeTok{Pr =}\NormalTok{ test\_stats[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(\textgreater{}|t|)"}\NormalTok{])}

\CommentTok{\# Create a table using kable}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(result\_df)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
& Estimate & Std\_Error & t\_value & Pr \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Estimate & -15204.78 & 1154.614 & -13.16871 & 0 \\
\end{longtable}

\pagebreak

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  (20 p) These questions pertain to the specification in expression
  (\ref{CFnc}) thus you obtain the
  \textcolor{ForestGreen}{Regression-Adjusted Treatment-Control Comparison (Adj. TCC) Estimator}
  of the treatment effect of the offer of training. \label{item:CFnc}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    (10 p) Add to the model estimated in \textbf{\ref{item:TCcomp}} the
    following OPVs as regression covariates: \texttt{age},
    \texttt{agesq}, \texttt{edu}, \texttt{nodegree}, \texttt{black},
    \texttt{hisp}, \texttt{re74}, and \texttt{re75}. Report
    \(\hat{\rho}\) and its heteroschedasticity-robust SE.
    \textcolor{gray}{\textbf{Programming Guidance:} Add column \texttt{agesq} (\texttt{age} squared) to your dataframe using, e.g., \texttt{dplyr::mutate( )}.}\label{item:CFnc-rho}
  \item
    (10 p) Intuitively explain why the Adj. TCC approach may be regarded
    as an improvement over the TCC approach when it comes to credible
    identification/estimation of average treatment effects.
  \end{enumerate}
\end{enumerate}

\pagebreak

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  (20 p) Consider again the specification in expression (\ref{CFnc})
  estimated in \textbf{\textbf{\ref{item:CFnc}}}. Here you implement two
  procedures, as detailed below, to verify the
  \textcolor{ForestGreen}{``partialling-out'' interpretation} of OLS
  coefficients in MLRM.\label{item:CFnc-po}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    (8 p) Procedure A:

    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \item
      (4 p) First Stage: Regress \texttt{treat} on a constant and the
      OPVs listed in \textbf{\ref{item:CFnc-rho}}; obtain the residuals.
      \textcolor{gray}{\textbf{Programming Guidance:} If you run \texttt{s1 $<$- lm(treat $\sim$ x1 + x2, data = dt)}, retrieve the residuals as \texttt{s1\$residuals}.}\label{item:CFnc-po-1ststep}
    \item
      (4 p) Second Stage: Regress \texttt{re78} on a constant and the
      residuals from
      \textbf{\ref{item:CFnc-po-1ststep}}.\label{item:CFnc-po-2ndstep}
    \end{enumerate}
  \item
    (8 p) Procedure B:\label{item:CFnc-po-procB}

    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \item
      First Stage: Same as
      \textbf{\ref{item:CFnc-po-1ststep}}.\label{item:CFnc-po-1ststep-bis}
    \item
      (4 p) First Stage: Regress \texttt{re78} on a constant and the
      OPVs listed in \textbf{\textbf{\ref{item:CFnc-rho}}}; obtain the
      residuals.\label{item:CFnc-po-3rdstep}
    \item
      (4 p) Second Stage: Regress the residuals from
      \textbf{\ref{item:CFnc-po-3rdstep}} on the residuals from
      \textbf{\ref{item:CFnc-po-1ststep-bis}}.\label{item:CFnc-po-4thstep}
    \end{enumerate}
  \item
    (4 p) Verify that the estimates of the slope coefficient from
    \textbf{\ref{item:CFnc-po-2ndstep}} and
    \textbf{\ref{item:CFnc-po-4thstep}} are numerically identical to
    \(\hat{\rho}\) obtained in \textbf{\ref{item:CFnc-rho}}. Use this
    fact to give meaning to the expression ``partialling-out'\,'
    interpretation of OLS in a MLRM.
    \textcolor{gray}{\textbf{Hint}: Think about what steps \textbf{\ref{item:CFnc-po-1ststep}} and \textbf{\ref{item:CFnc-po-3rdstep}} accomplish.}
  \end{enumerate}
\end{enumerate}

\pagebreak

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  (20 p) Consider the
  \textcolor{ForestGreen}{partially-linear specification} in expression
  (\ref{PLR}). Here you estimate \(\rho\) via the the
  \textcolor{ForestGreen}{Double Machine Learning (DML)} estimation
  procedure of Robinson (1988), as detailed below.\label{item:DML}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    (2 p) Install four R packages:
    \href{https://docs.doubleml.org/stable/intro/install.html#r-installing-doubleml}{\texttt{DoubleML}},
    \href{https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html}{\texttt{data.table}},
    \href{https://mlr3.mlr-org.com/}{\texttt{mlr3}}, and
    \href{https://mlr3learners.mlr-org.com/}{\texttt{mlr3learners}}.
  \item
    (2 p) If your data is not already a \texttt{data.table} object
    convert it.
    \textcolor{gray}{\textbf{Programming Guidance:} Assuming that your dataframe is called \texttt{df}, use \texttt{dt <- data.table::as.data.table(df)}. \texttt{data.table} is an extension of \texttt{data.frame} and allows for fast manipulation of very large data.}
  \item
    (2 p) Collect all the original OPVs in a list named, for example,
    \texttt{pretreat\_colnames}. Note: Henceforth when we refer to these
    OPVs in mathematical expressions we use the notation
    \(\mathbf{x}_{i}\).
  \item
    (2 p) Specify data and variables for the causal model by running the
    script:\label{item:dml-data}

\begin{verbatim}
dml_data_psid <- DoubleML::DoubleMLData$new(dt,
                        y_col = "re78",
                        d_cols = "treat",
                        x_cols = pretreat_colnames)
\end{verbatim}
  \item
    (2 p) Suppress messages from the \texttt{mlr3} package by adding

    \texttt{lgr::get\textbackslash{}\_logger("mlr3")\textbackslash{}\$set\textbackslash{}\_threshold("warn")}

    to your script.
  \item
    (2 p) Here you mimic the first stage of Procedure B in
    \textbf{\ref{item:CFnc-po-procB}}. Namely, you specify the model for
    the two regression functions
    \(l(\mathbf{x})=E[\texttt{re78}_i|\mathbf{x}_{i}=\mathbf{x}]\) and
    \(m(\mathbf{x})=E[\texttt{treat}_i|\mathbf{x}_{i}=\mathbf{x}]\). In
    \textbf{\ref{item:CFnc-po-procB}} you used a linear-in-parameter
    model and a priori decided which OPVs to include and which
    transformations to apply to the OPVs to include (e.g., you excluded
    \texttt{u74}, you used both \texttt{age} and \texttt{agesq}, you
    left as-is the other included OPVs). Instead here you do not a
    priori exclude any OPVs, and you use flexible models, which
    accommodate complex non-linearities. Run the
    script:\label{item:dml-first-stage-models}

\begin{verbatim}
# Specify a RF model as the learner model for l(x)=E[re78|X=x]
ml_l_rf <- mlr3::lrn("regr.ranger")

# Specify a RF model as the learner model for m(x)=E[treat|X=x]
ml_m_rf <- mlr3::lrn("classif.ranger")
\end{verbatim}
  \end{enumerate}

  The above script uses a
  \textcolor{ForestGreen}{Random Forest (RF) model} for both conditional
  expectations functions.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{6}
  \item
    (2 p) Here you initialize \& parametrize the model object which you
    later use to perform estimation. Run the script:

\begin{verbatim}
# Set seeds for cross-fitting
set.seed(3141)

# Set the DML specification
obj_dml_plr <- DoubleML::DoubleMLPLR$new(dml_data_psid, 
                                        ml_l = ml_l_rf, ml_m = ml_m_rf, 
                                        n_folds = 2,
                                        score = "partialling out",
                                        apply_cross_fitting = TRUE)
\end{verbatim}

    The above script: (i) utilizes the data object generated in
    \textbf{\ref{item:dml-data}}, namely \texttt{dml\_data\_psid}; (ii)
    utilizes the models for the first stage regressions picked in
    \textbf{\ref{item:dml-first-stage-models}}, namely
    \texttt{ml\_l\_rf} and \texttt{ml\_m\_rf}; (iii) specifies that we
    want to split the sample into 2 parts (\texttt{n\_folds = 2}), and
    (iv) that we want to use the ``partialling out'\,' approach to
    estimate causal impacts (\texttt{score = "partialling out"}), and
    (v) that we want to apply \textcolor{ForestGreen}{cross-fitting}
    (\texttt{apply\_cross\_fitting = TRUE}).\label{item:dml-model}
  \item
    (2 p) Here you fit the DML model defined in
    \textbf{\ref{item:dml-model}}. Run the script:

\begin{verbatim}
obj_dml_plr$fit()
obj_dml_plr
\end{verbatim}
  \end{enumerate}

  At a high level the above script implements all of the following
  operations: (i) fits the two models for the first stage selected in
  \textbf{\ref{item:dml-first-stage-models}}, (ii) gets residuals, (iii)
  regresses the residuals for the outcome variables onto the residuals
  for the treatment indicator to obtain the DML estimate of \(\rho\) in
  expression (\ref{PLR}). Note: You specified \texttt{n\_folds = 2} and
  requested \texttt{apply\_cross\_fitting = TRUE} in
  \textbf{\ref{item:dml-model}} thus the 2-stage estimation procedure
  proceed as follows. First the entire data is split into two
  sub-samples, call them A and B (hence the term
  \texttt{2\ folds\textquotesingle{}\textquotesingle{}).\ Sample\ A\ is\ used\ to\ fit\ the\ 1st\ stage\ models.\ These\ fitted\ models\ are\ used\ to\ compute\ residuals\ in\ sample\ B\ and\ these\ residuals\ are\ used\ to\ fit\ the\ 2nd\ stage\ model\ using\ only\ data\ in\ sample\ B.\ Denote\ the\ resulting\ estimate\ \$\textbackslash{}hat\{\textbackslash{}rho\}\_\{AB\}\$.\ Then\ the\ samples\ are\ swapped\ (hence\ the\ term}cross
  fitting'\,'). That is, sample B is used to fit the 1st stage models.
  Sample A is used to fit the 2nd stage model. Denote the resulting
  estimate \(\hat{\rho}_{BA}\). The DML estimate is the average of
  \(\hat{\rho}_{AB}\) and \(\hat{\rho}_{BA}\).

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    (4 p) Take a look at the output, i.e., at the object
    \texttt{obj\_dml\_plr}. How does the DML estimate of average
    treatment effect compare to the estimates based on specifications
    (\ref{TCcomp}) and (\ref{CFnc})?
  \end{enumerate}
\end{enumerate}

\end{document}
